[
  {
    "objectID": "backg-educ.html",
    "href": "backg-educ.html",
    "title": "Case-study 3: Literacy in 19th-century Spain",
    "section": "",
    "text": "This case-study relies on the report published by authorities that summarised the information collected in the 1860 Spanish Population Census. In particular, it focuses on the number of men and women living in each Spanish district (partido judicial) classified according to whether they were illiterate, able to read or able to read and write.\nAs the figure above illustrates, this source presents mostly numerical information, neatly structured in rows and columns, that can be easily transferred to a digital version. The sources contains information on the number of municipalities and households in each district, how many household (or the number the total number of men and wom\nThis is a very simple file reporting the number of individuals\nInputing the raw data into an Excel spreadsheet results in Figure 2 below. Each column, known as field or variable presents a piece of information. As well as the case number (casen) and the date of admission (information that is split in two fields: month and year), the source records several pieces of information about these inmates, such name and surname, sex, age, place of birth (born) and country of birth (countryb), the place where the were living before being imprisoned (reside), height (in feet and inches) and weight, occupation (occup) and whether they were employed or not. It also reports their literacy, the marks that were visible in their bodies, the offence they committed and the sentence they received. While the first row displays the name of these variables, the remaining rows are devoted to each individual in the dataset.\nWho were these prisoners? Where they were coming from? Did prisoners’ occupations differ significantly from the rest of the population? What about literacy rates? Did men and women commit different crimes? Did judges treat everyone equally or did particular groups suffer harsher sentences? What explains the variation in stature and weights observed across prisoners? How did theses dimensions change during the period? The range of historical questions that this source can address is almost endless. Sarah Horrell, David Meredith and Deborah Oxley have relied on this information to significantly contribute to our understanding of 19th-century British society, especially regarding the biological living standards of the working classes and the gender dynamics that drove the allocation of resources within these families [Horrell and Oxley (2013); Meredith and Oxley (2015)).1 We strongly encourage reading those pieces to get to know more about the source and its possibilities. Bear in mind that, for practical reasons, we will rely on a subset of the full data set, containing a thousand prisoners.2\nThis type of source allows exploring many historical questions:\nDistrict-level information (464 observations; partido judicial) on:"
  },
  {
    "objectID": "backg-educ.html#footnotes",
    "href": "backg-educ.html#footnotes",
    "title": "Case-study 3: Literacy in 19th-century Spain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn a seminal paper, Sarah Horrell and Deborah Oxley had previously addressed these issues using similar registers from the Wandsworth prison, near London (Horrell, Meredith, and Oxley 2009).↩︎\nWe are extremely grateful to Deborah Oxley and Hamish Maxwell-Stewart for kindly sharing the Paisley dataset.↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "By allowing researchers to integrate spatial dimensions into their analyses, digital mapping and GIS tools ultimately enhance our understanding of the phenomenon we are studying.\nThis course focuses on:\n\n\nCreating quality maps using polygons, points or pixels\n\n\nDiscussing Coordinate Reference Systems (CRSs)\n\n\nCreating your own spatial features\n\n\nUsing existing GIS files\n\n\nGeo-coding\n\n\nDigitising your own maps: old maps or aerial / satellite images (QGIS)\n\n\n\n\nImplementing spatial analysis: density (heat) maps, spatial correlation\n\n\nComputing variables using geo-spatial information (geo-computation)\n\n\nCalculating distances\n\n\nImplementing spatial joins\n\n\nComputing zonal statistics\n\n\n\n\nThe course mostly relies on R but will also introduce QGIS."
  },
  {
    "objectID": "paisley.html",
    "href": "paisley.html",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "",
    "text": "This case-study introduces basic tools to systematize and extract information from historical sources, regadless it is qualitative or numerical. Displaying frequency tables, plotting histograms and reporting summary statistics (i.e. the mean, the minimum and maximum values, the standard deviation, etc.) helps characterising how our data looks like. Making sense of the source using descriptive statistics can actually get you a long way in your understanding of the historical setting you are studying."
  },
  {
    "objectID": "paisley.html#setting-the-stage",
    "href": "paisley.html#setting-the-stage",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Setting the stage",
    "text": "Setting the stage\nAs explained in the Intro to R section, we need to include some preliminary commands in our script so we (1) get rid of other objects that could be in the R environment from previous sessions, (2) set the working directory, (3) load (and install if necessary) the packages we pla to use, and (4) import the dataset.\n\n# Clear de \"Global Environment\"\nrm(list=ls()) \n\n# Sets the working directory\nsetwd(\"~/Documents/quants\") \n\n# Install/load packages\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(readxl)\ndata &lt;- read_excel(\"data/paisley-data.xlsx\")"
  },
  {
    "objectID": "paisley.html#inspecting-the-data",
    "href": "paisley.html#inspecting-the-data",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nOnce the data is imported into the R environment as an object, we can start inspecting it as shown in the Intro to R section. As shown in the Global Environment, in the upper left corner, this this data frame (referred to as a tibble in the tidyverse terminology) contains 1,000 individuals (rows) and 21 fields (columns). Typing the name of the object (data) only provides information on the 10 first cases in order to save memory and space. Notice also that, below the variable name, R also indicates the type of variable: some are categorical (“chr” meaning character) and others are numerical (“dbl” meaning double).\n\ndata\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n# ℹ 990 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nIf you want to display more cases, you can use the function print(n = 15) and indicate the number of cases to be reported. Let’s pause for a moment to disect what the code below is doing. Notice first that it is crucial to indicate where the information you are asking for is coming from. Remember that we imported the Paisley data into the object name data. The pipe (|&gt;) here basically takes this object and uses it as input in the next line of code, which uses the function print() to request listing the first 15 cases from that object. Alternatively, you can have a look at the last 20 cases by typing tail(20).\n\ndata |&gt;  \n  print(n = 15)\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n11    11   343 may       1841 AGNES    CURRIE… fema…    35 islay scotland paisl…\n12    12   382 june      1841 ELLIZA   MUNN    fema…    18 john… scotland johns…\n13    13   425 june      1841 SARAH    BLACK … fema…    36 glas… scotland kelvi…\n14    57     3 january   1841 THOMAS   ROBERT… male     19 pais… scotland high …\n15    58    19 january   1841 JOHN     MONTGO… male     24 some… england  barra…\n# ℹ 985 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nThere are other commands that help knowing more about how the data set looks like such as glimpse() or names(). Feel free to try them out yourself. You can also have a sense of the magnitude and complexity of the whole data set by typing view(data). The latter opens up a new tab where you can explore the full data set at ease. Notice also that some values are missing. R signals them as NA (not available), meaning that no information is recorded in those fields for those observations. We will discuss the importance of missing values in due time.\n\nview(data)\n\nWhat it is important to stress now is that, although the Paisley data set is not especially big, scrolling up-down and left-right makes it obvious that it is extremely difficult to extract any kind of pattern by just “looking” at all this information. Here is where statistics (and R) come to the rescue."
  },
  {
    "objectID": "paisley.html#categorical-qualitative-variables",
    "href": "paisley.html#categorical-qualitative-variables",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Categorical (qualitative) variables",
    "text": "Categorical (qualitative) variables\nWe will start extracting information from the data set by focusing on categorical (qualitative) variables, those defined with words instead of numbers, such as sex, country of birth, occupation, etc. Each of these variables can exhibit certain values (categories) and it is important to stress that the difference between these values is merely qualitative (one category is no more or better than any other).\nThe first step is to assess how the distribution of values (categories) looks like, that is, to quantify their relative importance. A frequency table reports the number of observations (usually referred to as n) falling into each category, an information that can easily be retrieved using count() and indicating which variable you want to get information on.\n\ndata |&gt;  \n  count(sex)\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female   284\n2 male     716\n\n\nAs you will have guessed, the code above takes the object data that contains the Paisley data set and implements the function count() on the field sex. The results shows that the data contains 284 female prisoners and 716 male prisoners (the categories are presented in alphabetical order: “female” and “male”).\nWe could do the same with any other variable. Notice that the function count() also reports the number of missing values (if any). Reporting a frequency table of the variable employed yields three different categories: employed, unemployed and NA. In this case, the data set did not record the emplyment status of 821 prisoners. Analysing variables with missing values presents especial challenges because their accuracy depends on the reasons behind its “missingness”.\n\ndata |&gt;  \n  count(employed)\n\n# A tibble: 3 × 2\n  employed       n\n  &lt;chr&gt;      &lt;int&gt;\n1 employed     101\n2 unemployed    78\n3 &lt;NA&gt;         821\n\n\nAs with other commands listing information, count() only reports the first 10 categories by default. Some variables, such as occupation (occup) has many categories, so you probably want to inspect all of them. You can just indicate explicitly how many categories you want to display by using the function print(): while indicating n = 15 displays information on the first 15 categories, typing n = Inf reports all rows. Notice also that this command is now 3 lines long and we are using the pipe (|&gt;) in each line to implement this sequence of instructions: the pipe takes the output from that line and uses it as an input in the following line.\n\ndata |&gt;  \n  count(occup) |&gt;\n  print(n = 15)\n\n# A tibble: 209 × 2\n   occup             n\n   &lt;chr&gt;         &lt;int&gt;\n 1 at school         1\n 2 baker            15\n 3 barber            1\n 4 black smith      10\n 5 blacksmith        1\n 6 bleacher         13\n 7 block maker       1\n 8 block print       1\n 9 block printer     1\n10 boat builde       1\n11 boatman           7\n12 boatyard          1\n13 boiler make       3\n14 boiler maker      1\n15 boilermaker       2\n# ℹ 194 more rows\n\n\nAs the previous example using occupations show, frequency tables are not that useful when the variable contains a large number of categories. Also, by default, count() reports the different categories in alphabetical order which is often not particularly useful. In such case, it is better to present the categories according to their relative importance in order to quickly identify the most important categories. This is achieved by introducing the option sort and set it up as TRUE. The results indicate that the most common occupation was “labourer”, followed by “prostitute” and the rest. We also have 22 prisoners whose occupation was not recorded. You may wonder whether they did not want to report it or they did not have one. The same output can be achieved using the function arrange(desc(n)), which enables presenting the data in descending order based on the column n.\n\ndata |&gt;  \n  count(occup, sort = TRUE)\n\n# A tibble: 209 × 2\n   occup            n\n   &lt;chr&gt;        &lt;int&gt;\n 1 labourer       181\n 2 prostitute      81\n 3 weaver          40\n 4 carter          28\n 5 hawker          28\n 6 miner           28\n 7 house keeper    23\n 8 seaman          22\n 9 &lt;NA&gt;            22\n10 house keepe     16\n# ℹ 199 more rows\n\n\nThese simple descriptions of the data not only allow identifying what is typical, but also what is rare. In this regard, we can use arrange() to report those occupations that are less common among prisoners. Note that, by default, arrange() sorts the data in ascending order, so this command only needs the name of the field that serves to sort the data in ascending order (in the previous examples, we explicitly indicated that we wanted the data to be presented in descending order using the desc() argument). As seen below, it is also possible to extend the pipe (|&gt;) and use print() to report more categories if needed.\n\ndata |&gt;  \n  count(occup) |&gt; \n  arrange(n) |&gt; \n  print(n=15) \n\n# A tibble: 209 × 2\n   occup              n\n   &lt;chr&gt;          &lt;int&gt;\n 1 at school          1\n 2 barber             1\n 3 blacksmith         1\n 4 block maker        1\n 5 block print        1\n 6 block printer      1\n 7 boat builde        1\n 8 boatyard           1\n 9 boiler maker       1\n10 boot binder        1\n11 brick maker        1\n12 brothel keeper     1\n13 butcher            1\n14 cab driver         1\n15 cabinet making     1\n# ℹ 194 more rows\n\n\nLikewise, apart from the absolute number of observations falling in each category, it is useful to report the relative frequency. Frequency tables in fact routinely report both values. As explained above, count() effectively transforms the paisley data set into something different by summarising the info contained in a particular field. If we want to report the relative frequency, we need to add another column computing it. Therefore, we need to create a new variable using the command mutate() and instruct R how to populate that new field. As you will soon find out, mutate() is an extremely important command. In the example below, it takes the table created by count() as input and creates a new variable named prop (or something else; you decide which name you give to the new field). The value of the new field is something that the researcher sets up. In this case, we want to compute the relative frequency of each occupational category, that is, the number of cases falling into each category divided by the total number of observations. While the first value is the result of the function count() as reported in the field n, the second value can be retrieved by using the appropriate function. The operation n/sum(n) will therefore achieve what you need.\n\ndata |&gt;  \n  count(occup, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) \n\n# A tibble: 209 × 3\n   occup            n  prop\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt;\n 1 labourer       181 0.181\n 2 prostitute      81 0.081\n 3 weaver          40 0.04 \n 4 carter          28 0.028\n 5 hawker          28 0.028\n 6 miner           28 0.028\n 7 house keeper    23 0.023\n 8 seaman          22 0.022\n 9 &lt;NA&gt;            22 0.022\n10 house keepe     16 0.016\n# ℹ 199 more rows\n\n\nCrucially, you can narrow your analysis by focusing on particular subsamples of the data (or excluding outliers). This is achieved using filter() which allows “filtering” the data set according to the conditions that you specify. Imagine, for instance, that you are interested in knowing the educational background of the female prisoners. By specifying that the variable sex should be equal to “female”, filter() restricts the analysis to those observations (rows) fulfilling this condition. The results below show that, while many women in the Paisley data are either “illiterate” or “read a little”, only 7 “read & write well”.\n\ndata |&gt; \n  filter(sex==\"female\") |&gt;   \n  count(literacy, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) \n\n# A tibble: 10 × 3\n   literacy                   n    prop\n   &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;\n 1 read a little             94 0.331  \n 2 illiterate                79 0.278  \n 3 read & write a little     61 0.215  \n 4 &lt;NA&gt;                      18 0.0634 \n 5 cannot write              11 0.0387 \n 6 read & write well          7 0.0246 \n 7 read & write tolerably     5 0.0176 \n 8 read well                  4 0.0141 \n 9 read tolerably             3 0.0106 \n10 superior education         2 0.00704"
  },
  {
    "objectID": "paisley.html#numerical-variables",
    "href": "paisley.html#numerical-variables",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Numerical variables",
    "text": "Numerical variables\nLet’s now explore variables that are expressed using numerical values. In the Paisley dataset, we only have three numerical variables: age, height and weight. Their specific properties advice to analyse them using a wider set of statistics. In this regard, simply reporting frequencies is often not very useful, especially when these variables include a large range of values. See, for instance, what happens when construct frequency table using now the variable age. The results report the number of observations falling in each category: 2 prisoners aged 9 years, 3 aged 10, etc. (we have also used mutate() to create an additional column with the relative frequency expressed as percentages). For questions of space, we just report the first 10 rows but the point is clear. You can display the full table, containing 62 rows, by adding the option print(n=Inf). The full table obviously provides interesting insights but it is simply too large for being useful as an interpretative tool.\n\ndata |&gt;  \n  count(age) |&gt;              \n  mutate(perc = 100*n/sum(n))\n\n# A tibble: 62 × 3\n     age     n  perc\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1     9     2   0.2\n 2    10     3   0.3\n 3    11     4   0.4\n 4    12     9   0.9\n 5    13     6   0.6\n 6    14    18   1.8\n 7    15    17   1.7\n 8    16    18   1.8\n 9    17    30   3  \n10    18    48   4.8\n# ℹ 52 more rows\n\n\nA useful way of exploring and reporting numerical variables is by using a histogram. This type of plot provides a visual representation of the distribution of values of the variable that we are analysing. The command ggplot() easily allows constructing histograms. You first need to indicate which variable you want to depict in the x-axis and then decide which type of plot you want. We have also included a line of code that establishes how the x-axis is labelled (in this case in multiples of 10 starting at age 0 and ending at age 90). As with any other function, we can also first use filter() to narrow down the analysis to particular subsamples of our data (i.e. gender, country of birth, etc.).\n\ndata |&gt;   \n  ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 5) +\n    scale_x_continuous(breaks = seq(0, 90, 10))\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHistograms present visual representations of the distribution of numerical variables. They are extremely useful because they provide an all-encompassing view of the data under analysis. However, it is also important to report specific values that help accurately describing the distribution. Descriptive statistics reduce complex distributions to more simple and intelligible numbers, thus making comparing distributions easier. The mean (or the average) is the most popular descriptive statistic but, depending on the researcher’s aim, other statistics may prove even more important.\nThe command summarise() allows computing all these descriptive statistics, also known as summary statistics. The following, for instance, compute the prisoners’ average age. This computation will generate a variable (we have assigned it here the name mean_age but you can choose any other name) that is equal to the function we specify. In this case, we want to compute the average, so we use the function mean(). Notice that we are also including the parameter na.rm = TRUE, which stands for “NA remove”, in order to exclude missing values from the computations and ensure accurate results (otherwise it results in NA because it cannot be computed; you can check the results removing that condition). As shown below, our prisoners are relatively young, on average.\n\ndata |&gt; \n  summarize(mean_age = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_age\n     &lt;dbl&gt;\n1     29.6\n\n\nCalling different functions within summarize() allows calculating other statistics. In the example below, we also compute the number of observations with information of age and the minimun and maximum values. We are using the functions sum(), mean(), min() and max() to compute the corresponding statistics.1 This exercise tells us that the average age (mean) of the 999 prisoners reporting age (obs) is 29.6 years. It also indicates that there is at least a prisoner as young as 9 years old (min) and at least another one as old as 89 (max).\n\ndata |&gt; \n  summarize(\n    obs = sum(!is.na(age)),\n    mean = mean(age, na.rm = TRUE), \n    min = min(age, na.rm = TRUE),\n    max = max(age, na.rm = TRUE)) \n\n# A tibble: 1 × 4\n    obs  mean   min   max\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   999  29.6     9    89"
  },
  {
    "objectID": "paisley.html#bivariate-statistics",
    "href": "paisley.html#bivariate-statistics",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Bivariate statistics",
    "text": "Bivariate statistics\nHistorians and other social scientists routinely base their narratives on comparisons across different dimensions (gender, age, socio-economic groups, regions, etc.). Let’s then try to describe two variables simultaneously.\nImagine, for instance, that we want to provide information on age and adult weight. Notice that weight is expressed in pounds. For those of us more used to deal with the metrical system, it is advisable to change the unit of measurement. This can be done by creating another variable (weight_kg) that makes the conversion (1 kgs. = 0.453592 pounds; the operator &lt;- instruct R to modify the object data accordingly).\n\ndata &lt;- data |&gt;\n  mutate(weight_kg = 0.453592*weight)\n\nComing back to our main purpose of simultaneously looking at age and weight, it would not make sense to report a table listing the average height for each age (i.e. 20, 21, 22, etc.), so we first use mutate() to create a variable grouping age into different class intervals (in 10-year cohorts starting at age 9) and then compute the average for each group using group_by() and summarise(). Given that we want to focus on adult weight, we are restricting the analysis to those age 20 or older. The results clearly show that older prisoners tend to have lower weights on average.2\n\ndata |&gt; \n  filter(age&gt;=20) |&gt;\n  mutate(age_class = cut(age, breaks = seq(19, 89, 10))) |&gt;  \n  group_by(age_class) |&gt;\n  summarise(obs = sum(!is.na(weight_kg)),\n            mean_weight = mean(weight_kg, na.rm = TRUE))\n\n# A tibble: 7 × 3\n  age_class   obs mean_weight\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 (19,29]     331        63.7\n2 (29,39]     167        62.4\n3 (39,49]     104        62.6\n4 (49,59]      49        59.7\n5 (59,69]      21        57.2\n6 (69,79]       4        55.1\n7 (79,89]       1        44.5\n\n\nThe same information could be depicted using a line graph by using geom_line() in ggplot() and indicating to plot the variables we just computed (age and mean_weight) in the x- and y-axes, respectively. Notice that instead of grouping ages into 10-year intervals, we are using the full distribution of ages (the trade-off is the higher variation arising from the low number of observations for each individual age). Contrary to long tables containing all the categories, plots allow presenting all the information in a more concise way. The graph below tracks the relationship between these two dimensions and clearly suggests that getting older is associated with losing weight (or, at least, that the older prisoners in our data set are lighter than the younger ones).\n\ndata |&gt; \n  filter(age&gt;=20) |&gt;\n  group_by(age) |&gt;\n  summarise(mean_weight = mean(weight_kg, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = age, y = mean_weight, group = 1)) +\n    geom_line()\n\n\n\n\n\n\n\n\nWe could obviously refine our analysis to take into account other dimensions of our data. Using filter(), for instance, would allow to focus on particular subsamples of our data (i.e. males or females) or exclude outliers, that is, observations with extreme values that may distort our results (i.e. very old prisoners; see chapter X for a more detailed discussion of outliers). We can also go beyond the previous graph and simultaneously consider other dimensions in the visualisation itself. The code below replicates the previous plot but distinguishing by sex and excluding the prisoner who are really old (80+). This exercise not only makes clear that women are in general lighter, but also that they seem to lose weight more rapidly than men: the slope of the line tracking the relationship between age and weight is steeper.\n\ndata |&gt;\n  filter(age&gt;=20 & age&lt;80) |&gt;\n  group_by(age, sex) |&gt;\n  summarise(mean_weight = mean(weight_kg, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = age, y = mean_weight, color = sex)) +\n    geom_point() +\n    geom_line()\n\n\n\n\n\n\n\n\nWe don’t have time for more. This is though just the very tip of the iceberg. Quantitative tools allow extracting information from historical sources in a powerful way, regardless whether the information is numerical or qualitative."
  },
  {
    "objectID": "paisley.html#footnotes",
    "href": "paisley.html#footnotes",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is important to notice the comma after each function since it allows for computing additional statistics (the last one thus does not need the comma). Note also that the brackets need to be balanced. Likewise, the function requesting the number of observations (sum()) is structured differently than the rest. This is because we need to count the number of prisoners reporting age (the option !is.na effectively ask to only consider those observations who are not reporting a missing value in the variable age). If we had used the code sum(age), na.rm = TRUE as in the other command lines, we would have obtained the sum of all the prisoners’ ages (29,562 years; you can test it yourself).↩︎\nThere are only a few very really old prisoners, so their average weight is also very much influenced by chance.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital mapping for the humanities and the social sciences - HIST8872",
    "section": "",
    "text": "February 2026 – 09:15-16:00 – TBC (Akrinn, Kalvskinnet)\nInstructor: Francisco J. Beltrán Tapia\nThis course, offered by the Faculty of Humanities at the Norwegian University of Science and Technology, provides a practical introduction on how to apply Geographic Information Systems and its associated methods in the humanities and the social sciences.\n\n\n\n\n\nThe cholera epidemic in Soho (London), 1854 (source).",
    "crumbs": [
      "Overview",
      "Contents"
    ]
  },
  {
    "objectID": "index.html#welcome-to-digital-mapping",
    "href": "index.html#welcome-to-digital-mapping",
    "title": "Digital mapping for the humanities and the social sciences - HIST8872",
    "section": "",
    "text": "February 2026 – 09:15-16:00 – TBC (Akrinn, Kalvskinnet)\nInstructor: Francisco J. Beltrán Tapia\nThis course, offered by the Faculty of Humanities at the Norwegian University of Science and Technology, provides a practical introduction on how to apply Geographic Information Systems and its associated methods in the humanities and the social sciences.\n\n\n\n\n\nThe cholera epidemic in Soho (London), 1854 (source).",
    "crumbs": [
      "Overview",
      "Contents"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Digital mapping for the humanities and the social sciences - HIST8872",
    "section": "Description",
    "text": "Description\nGeo-spatial information is crucially shaping our understanding of the world we live in. GIS tools are being rapidly adopted by local and national policy makers and the business sector. Likewise, not only the humanities and the social sciences need to explicitly consider the spatial dimension of economic and social processes, but thinking spatially facilitates the adoption of innovative research techniques. Scheduled over four three-hour lab sessions, this hands-on course provides a thorough overview of how to map spatial information and apply GIS tools in empirical studies using R, a free programming software widely used by practitioners in many different fields.\nThe course is taught intensively in four 3-hour sessions over two days (morning and afternoon sessions; 12 hours in total). Each session combines lectures and computer practicals. No previous background in computing or statistics is required.\n\nImportant warning\nPlease follow the Instructions section to prepare for the course in advance. As well as reading the article Quantifying history, this involves downloading the course materials, installing and getting familiar with R and RStudio and going through the background information on the case-studies we will be exploring.",
    "crumbs": [
      "Overview",
      "Contents"
    ]
  },
  {
    "objectID": "r-scripts/mapping.html",
    "href": "r-scripts/mapping.html",
    "title": "1.5. Mapping (spatial information)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n###### Mapping ######\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# upload basic packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\n# install.packages(\"sf\")\n# install.packages(\"tmap\")\n# install.packages(\"geodata\")\nlibrary(sf)\nlibrary(tmap)\nlibrary(geodata)\n\n\n# Examples\n\n# 1 Shapefiles\n\nmun_sh &lt;- read_sf(\"data/mapping/mun_1860_1930/mun_1860_1930.shp\")\ndist_sh &lt;- read_sf(\"data/mapping/educ_1860/dist_1860.shp\")\ncoast_sh &lt;- read_sf(\"data/mapping/coastline/spain_coastline.shp\")\n\ntm_shape(coast_sh, bbox = dist_sh) + tm_lines() +\n  tm_shape(dist_sh) + tm_borders() +\n  tm_shape(mun_sh) + tm_bubbles(col = \"red\", alpha = 0.5, border.col = \"red\", size = \"pop1860\", \n                                sizes.legend = c(250, 500, 1000, 5000, 10000, 20000, 50000, 100000, 500000),\n                                sizes.legend.labels = c(\"&lt;.25\", \".25-.50\", \".5-1\", \"1-5\", \"5-10\", \"10-20\", \"20-50\", \"50-100\", \"&gt;100\"),\n                                title.size = \"Population (in thousands), 1860\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), legend.frame = TRUE, legend.bg.color = TRUE)\n\n# 2 Raster files\n\nelev_spain &lt;- geodata::elevation_30s(country = \"ES\", res = 0.5, path = tempdir()) # SRTM 1KM\n\nbox_coord &lt;- list(rbind(c(-3.7, 36.6), c(-3.7, 37.4), c(-2.9, 37.4), c(-2.9, 36.6), c(-3.7, 36.6))) # close the polygon (draw points in order)\nbox &lt;- st_polygon(box_coord)\nbox_shp &lt;- st_sfc(box, crs = \"EPSG:4326\")\n\nm1 &lt;- tm_shape(elev_spain, bbox = c(-9.5, 36, 4.5, 44)) + \n  tm_raster(style = \"fixed\", breaks = c(0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500), \n            palette = terrain.colors(14)) +\n  tm_layout(legend.show = FALSE) +\n  tm_shape(box_shp) + tm_borders()\n\nm2 &lt;- tm_shape(elev_spain, bbox = c(-3.7, 36.6, -2.9, 37.4)) + \n  tm_raster(style = \"fixed\", breaks = c(0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500), \n            palette = terrain.colors(14), title = \"Elevation (mts.)\") +\n  tm_layout(legend.outside = TRUE, legend.outside.size = 0.5, legend.outside.position = \"right\", legend.text.size= .5)\n\ntmap_arrange(m1, m2, ncol = 2, asp = NULL, widths = c(.445, .555))\n\n## Spatial files are composed by a bunch of files\n  # R (or any other GIS software) treats them jointly\n\n##### Shapefiles: polygons, lines, points\n\n### Polygons\n\n# Import files\ndist_sh &lt;- read_sf(\"data/mapping/educ_1860/dist_1860.shp\") # Spanish districts\ndist_sh\n\n# Map it: t_shape() + tm_polygons() (or tm_borders, tm_fill...)\ndist_sh %&gt;% \n  tm_shape() +\n  tm_polygons(col = \"grey\", lwd = 0.5)\n\n# Map the info contained in particular fields (variables)\ndist_sh %&gt;%\n  tm_shape() +\n    tm_polygons(col = \"literacy_m\")\n\n?tm_polygons\n\ndist_sh %&gt;%\n  tm_shape() +\n    tm_polygons(col = \"literacy_m\", \n              breaks = c(0, 15, 30, 45, 60, Inf),\n              title = \"\") + # Remove the legend title\n    tm_scale_bar(position = c(\"right\", \"top\"))\n\n# Multiple maps\ndist_sh %&gt;%\n  tm_shape() +\n    tm_polygons(col = c(\"literacy_m\", \"literacy_f\"),\n              breaks = c(0, 15, 30, 45, 60, Inf),\n              title = \"Literacy (%)\") +\n    tm_layout(legend.outside = TRUE)\n\n# or creating different objects\n\nm1 &lt;- dist_sh %&gt;%\n  tm_shape() +\n    tm_polygons(col = \"literacy_m\", \n              breaks = c(0, 15, 30, 45, 60, Inf))\n\nm2 &lt;- dist_sh %&gt;%\n  tm_shape() +\n    tm_polygons(col = \"literacy_f\", \n              breaks = c(0, 15, 30, 45, 60, Inf))\n\ntmap_arrange(m1, m2, ncol = 2)\n\ntmap_arrange(m1, m2, ncol = 2) %&gt;%  \n  tmap_save(filename = \"output/map_lit_1860.png\", dpi = 600)\n  # save the map as a .png file (with resolution = 600)\n\n# Categorical (qualitative) variables\n\ndist_sh %&gt;%\n  tm_shape() +\n    tm_polygons(col = \"province\") +\n    tmap_options(max.categories = 48) +\n    tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n### Point shapefiles\n\nmun_sh &lt;- read_sf(\"data/mapping/mun_1860_1930/mun_1860_1930.shp\")\nmun_sh\n\n# Map it: tm_shape() + tm_dots() (or tm_bubbles...)\nmun_sh %&gt;%\n  filter(ccau!=5) %&gt;%\n  tm_shape() +\n    tm_dots(col = \"blue\", size = 0.01)\n\n# Adding contour for references\nspain &lt;- read_sf(\"data/mapping/ESP_adm0/ESP_adm0_pr_peninsula.shp\") # import Spanish boundaries\n  # import shapefile with the contour first\n\nmun_sh %&gt;%\n  filter(ccau!=5) %&gt;%\n  tm_shape() +\n    tm_dots(col = \"blue\", size = 0.01) + \n    tm_shape(spain) +\n    tm_borders()\n\n# Adjusting the size of the dots according to a particular field\n  # population in 1860 here (pop1860)\nmun_sh %&gt;%\n  filter(ccau!=5) %&gt;%\n  tm_shape() +\n  tm_bubbles(col = \"blue\", alpha = 0.5, border.col = \"blue\", \n             size = \"pop1860\",\n             sizes.legend = c(500, 1000, 5000, 10000, 20000, 50000, 100000, 500000),\n             sizes.legend.labels = c(\"&lt;.5\", \".5-1\", \"1-5\", \"5-10\", \"10-20\", \"20-50\", \"50-100\", \"&gt;100\"),\n             title.size = \"Population (in thousands), 1860\") + \n  tm_shape(spain) +\n  tm_borders()\n\n# Adding labels to the features: tm_text()\n\nmun_sh %&gt;%\n  filter(ccau!=5) %&gt;%\n  tm_shape() +\n  tm_dots(col = \"blue\", size = \"pop1860\") + \n  tm_text(\"municipio\", just = \"left\", xmod = 0.5, size = 0.8) +\n  tm_shape(spain) +\n  tm_borders()\n\n\ncities &lt;- mun_sh %&gt;% \n  filter(pop1860&gt;=50000) # big cities\ncities %&gt;%\n  tm_shape() +\n  tm_dots()\n\nmun_sh %&gt;%\n  filter(ccau!=5) %&gt;%\n  tm_shape() +\n    tm_dots(col = \"blue\", size = \"pop1860\") + \n    tm_shape(spain) +\n    tm_borders() +\n    tm_shape(cities) +\n    tm_text(\"municipio\", just = \"left\", xmod = 0.5, size = 0.8)\n\n# Temporal variation\n\nmun_sh %&gt;%\n  filter(ccau!=5) %&gt;%\n  tm_shape() +\n    tm_dots(col = \"blue\", size = c(\"pop1860\", \"pop1900\", \"pop1930\"),\n          sizes.legend = c(250, 500, 1000, 5000, 10000, 20000, 50000, 100000, 500000, 1000000, 2000000),\n          sizes.legend.labels = c(\"&lt;.25\", \".25-.50\", \".5-1\", \"1-5\", \"5-10\", \"10-20\", \"20-50\", \"50-100\", \"100-500\", \"500-1000\", \"&gt;1000\"),\n          title.size = \"Population (in thousands)\") + \n    tm_facets(nrow = 1, free.scales.symbol.size = FALSE) +\n    tm_layout(panel.labels = c(\"1860\", \"1900\", \"1930\"), panel.label.bg.color = \"white\",\n            legend.outside = TRUE, legend.position = c(\"center\", \"bottom\")) +\n    tm_shape(spain) +\n    tm_borders()\n\n\n# or using facets but we need to structure the data differently\nmun_sh_long &lt;- mun_sh %&gt;%\n  pivot_longer(cols = starts_with(\"pop\"), \n               names_to = \"year\", \n               names_prefix = \"pop\", \n               values_to = \"pop\") %&gt;%\n  filter(year==\"1860\" | year==\"1900\" | year==\"1930\") %&gt;% # select only the years I am interested in\n  filter(ccau!=5) # excluding Canarias\nmun_sh_long\n\ntm_shape(spain) + tm_borders() + # Spanish border\n  tm_shape(mun_sh_long) + # municipalities\n    tm_dots(col = \"blue\", size = \"pop\", title.size = \"Population\") + \n    tm_facets(by = \"year\", nrow = 1, free.scales.symbol.size = FALSE) +\n    tm_layout(legend.outside.position = \"bottom\")\n\n# Animated maps\n\n# install.packages(\"gifski\")\nlibrary(\"gifski\")\n\nmap_anim &lt;- tm_shape(spain) + tm_borders() + \n  tm_shape(mun_sh_long) + tm_symbols(size = \"pop\") +\n  tm_facets(by = \"year\", nrow = 1, ncol = 1, free.coords = FALSE)\n\ntmap_animation(map_anim, filename = \"output/pop_1860_1930.gif\", delay = 2) \n    # save as a gif\n\n\n### Raster data\nlibrary(geodata) # library containing ready-to-use spatial files (including rasters)\n?geodata\n\nelev_spain &lt;- geodata::elevation_30s(country = \"ES\", res = 0.5, path = tempdir()) # SRTM 1KM\nelev_spain\n\n# Map it: tm_shape() + tm_raster()\nelev_spain %&gt;%\n  tm_shape() + \n    tm_raster(title = \"Elevation\", palette = terrain.colors(14)) + \n    tm_legend(outside = TRUE)\n\n# Combining raster and shape files\nzgz_shp &lt;- read_sf(\"data/mapping/ESP_adm2/ESP_adm2.shp\") %&gt;% \n  filter(NAME_2==\"Zaragoza\")\n  # importing shapefile with province boundaries\n  # we already have a shapefile with municipalities\nrivers_main_shp &lt;- read_sf(\"data/mapping/rivers/A3_main.shp\")\nrivers_second_shp &lt;- read_sf(\"data/mapping/rivers/A3_secondary.shp\")\n\nelev_spain %&gt;% \n  tm_shape(bbox = zgz_shp) + \n    tm_raster(title = \"Elevation\", palette = terrain.colors(14)) + \n    tm_legend(outside = TRUE) + \n  tm_shape(rivers_main_shp) + tm_lines(col = \"blue\", lwd = 1) +\n  tm_shape(rivers_second_shp) + tm_lines(col = \"blue\", lwd = 0.5) +  \n  tm_shape(mun_sh) + tm_dots(size = \"pop1860\", title.size = \"Population, 1860\")\n\n\n\n### A brief note on coordinate systems and projections\n\n# install.packages(\"spData\")\nlibrary(spData)\n\nm0 &lt;- tm_shape(world, projection = 4326) + tm_polygons() + tm_credits(\"WGS 84\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4)\nm1 &lt;- tm_shape(world, projection = 8857) + tm_polygons() + tm_credits(\"Equal Earth\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4)\nm2 &lt;- tm_shape(world, projection = \"+proj=moll\", ) + tm_polygons() + tm_credits(\"Mollweide\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4) \nm3 &lt;- tm_shape(world, projection = \"+proj=wintri\", ) + tm_polygons() + tm_credits(\"Winkel Tripel\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4)\n\ntmap_arrange(m0, m1, m2, m3, ncol = 2)\n\n# distortions either in shape, area, distance or direction\n\n# Spatial objects usually have the adequate CRS already defined\n# Combining spatial objects with different CRSs is problematic\n\n## Some CRSs (authority:code)\n# WGS 84 (short for World Geodetic System 1984 (EPSG:4326)\n# WGS 84 / World Mercator (EPSG:3395) -- used by Google Maps\n# WGS 84 / Pseudo-Mercator (EPSG:3857)\n# LAEA Europe (EPSG:3035) -- Lambert Azimuthal Equal Area\n# UTM projections are especially suited for working with small areas. \n  # The earth is divided into 60 tiles (North/South the Equator). \n  # You should choose the one that covers your area of study. \n  # For Spain: ETRS 1989 UTM Zone 30N (\"EPSG:25830\").\n\n# check for help when choosing CRSs: \n# https://jjimenezshaw.github.io/crs-explorer/\n\n## Retrieving the CRS: authority:code -- summary()\nspain                         # ETRS89 / UTM zone 30N\nspain %&gt;% summary(\"geometry\") # epsg:25830\nspain %&gt;% st_crs()\n  # provides all the information needed to properly identify the CRS.\n\n## Changing the CRS\nspain2 &lt;- st_set_crs(spain, \"EPSG:3035\") # set CRS (LAEA Europe)\nspain2 %&gt;% summary(\"geometry\")\nspain2 &lt;- st_transform(spain, \"EPSG:3035\") # set CRS\n\n\nm0 &lt;- tm_shape(spain) + tm_polygons() + tm_credits(\"ETRS89 / UTM zone 30N\", position = c(\"RIGHT\", \"BOTTOM\"))\nm1 &lt;- tm_shape(spain2) + tm_polygons() + tm_credits(\"LAEA Europe\", position = c(\"RIGHT\", \"BOTTOM\"))\ntmap_arrange(m0, m1, ncol = 2)\n\n\n#### Mapping historical (or otherwise) data\n\n## Rely on existing GIS files\n# Search online for what you are looking for\n# The Historical GIS Research Network\n  # http://www.hgis.org.uk/resources.htm\n# Geospatial Historian\n  # https://geospatialhistorian.wordpress.com/finding-data/\n# Historical gazetteers\n  # World Historical Gazetteer: https://whgazetteer.org\n# Use contemporary files (and adapt them if necessary)\n  # Natural Earth: https://www.naturalearthdata.com/features/\n  # GADM: https://www.gadm.org (administrative boundaries)\n  # National agencies\n\n\n## (1) Import them using read_sf()\n  # regardless whether shapefiles are historical or contemporary\n  # use filter() if necessary to extract the features you are interested in\n\n## (2) merge them with the information you have gathered \n  # from other the archive or other sources\n\n## Illustration using Paisley\n\nlibrary(readxl) \npaisley &lt;- read_excel(\"data/paisley_data.xlsx\") # Paisley data\npaisley_born &lt;- paisley %&gt;%\n  filter(countryb==\"scotland\") %&gt;%\n  count(born, sort = TRUE)\npaisley_born\n\nlocations &lt;- read_sf(\"data/mapping/Localities2020centroids/Localities2020_Centroids.shp\") # import the spatial object (shapefile)\nlocations # shapefile with Scottish locations\n\nscotland &lt;- read_sf(\"data/mapping/scotland/scotland.shp\") # import the spatial object (shapefile)\n\nlocations %&gt;% summary(\"geometry\") # epsg:27700\nscotland %&gt;% summary(\"geometry\") # epsg:4326\nscotland &lt;- st_transform(scotland, \"EPSG:27700\") # set CRS\n\ntm_shape(scotland, bbox = locations) + tm_borders() +\n  tm_shape(locations) + tm_dots(col = \"blue\")\n\n## Clean the Paisley locations\npaisley &lt;- paisley %&gt;%\n  mutate(born = str_trim(born)) %&gt;%       # removes leading/trailing spaces\n  mutate(born = str_to_lower(born)) %&gt;%   # all to lower letters\n  mutate(born_adj = recode(born,          # homogenising categories\n                           \"campsey\" = \"campsie\",                     \n                           \"bridge of wier\" = \"bridge of weir\",\n                           \"n kilpatrick\" = \"new kilpatrick\"))\n\npaisley &lt;- paisley %&gt;%\n  mutate(born_adj = str_replace(born_adj, \"shire\", \"\")) # removing \"shire\"\n\npaisley_born &lt;- paisley %&gt;%\n  filter(countryb==\"scotland\") %&gt;%\n  count(born_adj)\npaisley_born\n\n# Merge both objects: shapefile - paisley places \nlocations_ext &lt;- locations %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%   # converts to lower case\n  left_join(paisley_born, by = join_by(name == born_adj))\nlocations_ext\n\nlocations_ext %&gt;%\n  filter(!is.na(n))\n\n# map the number of prisoners \n  # assuming we are satified with the matching\nscotland %&gt;%\n  tm_shape(bbox = locations_ext) + tm_borders() +\n  tm_shape(locations) + tm_dots(col = \"grey\") +\n  tm_shape(locations_ext) + tm_bubbles(col = \"red\", size = \"n\")\n  # most of our Scottish prisoners were born relatively near the prison\n\n## Adding XY coordinates: st_as_sf()\n\nlibrary(readxl) \nzgz_mun &lt;- read_excel(\"data/mapping/mun_zgz_1860.xlsx\")\nzgz_mun \n\nzgz_mun_shp &lt;- st_as_sf(zgz_mun, coords = c(\"lat\", \"lon\"), crs = 3042)\nzgz_mun_shp\n\nzgz_mun_shp %&gt;%\n  tm_shape() + tm_dots() +\n  tm_shape(zgz_shp) + tm_borders()\n\n\n## Geocoding\n\npaisley_born %&gt;% arrange(-n)\n\n# install.packages(\"tidygeocoder\")\nlibrary(tidygeocoder)\n\nplaces_geo &lt;- paisley_born %&gt;%\n  geocode(born_adj, method = \"osm\", lat = latitude , lon = longitude, full_results = TRUE)\nplaces_geo\nview(places_geo)\n\n  # the method refers to the geocoding service you are requesting\n    # `osm` refers to the *Open Street Map Nominatim API\n    # others: arcgis, census, google maps, etc.; \n    # see the package documentation:\n      # https://cran.r-project.org/web/packages/tidygeocoder/tidygeocoder.pdf\n      # the Google Maps Geocoding API requires an API key, so it might not be free\n\n  # some locations are not found\n  # others are found in other countries: US, Canada, Australia\n\n# improve the geocoding by adding more info (country)\n\npaisley_born &lt;- paisley_born %&gt;%\n  mutate(born_adj = str_to_title(born_adj)) %&gt;%            # capitalise the first letter\n  mutate(born_adj = paste(born_adj, \", Scotland\", sep = \"\"))  # add string\npaisley_born\n\nplaces_geo &lt;- paisley_born %&gt;%\n  geocode(born_adj, method = \"osm\", full_results = TRUE)\nplaces_geo\n\nplaces_geo %&gt;%\n  filter(is.na(lat))\n  # correct typos\n  # finding the coordinates (lat, long) manually and add them using mutate()\n\n# transform it into a spatial object (including CRSs)\nplaces_geo_sf &lt;- places_geo %&gt;%\n  filter(!is.na(lat)) %&gt;%\n  st_as_sf(coords = c(\"lat\", \"long\"), crs = \"EPSG:27700\") # WGS 84 4326 EPSG:27700\n  \n# map it\nplaces_geo_sf %&gt;%\n  tm_shape() + tm_bubbles(col = \"red\", size = \"n\") +\n  tm_shape(scotland) + tm_borders()\n\nscotland %&gt;%\n  tm_shape(bbox = places_geo_sf) + tm_borders() +\n  tm_shape(locations) + tm_dots(col = \"grey\") +\n  tm_shape(places_geo_sf) + tm_bubbles(col = \"red\", size = \"n\")\n\nscotland &lt;- st_set_crs(scotland, \"EPSG:27700\") # set CRS (LAEA Europe)\nscotland %&gt;% summary(\"geometry\")\nscotland &lt;- st_transform(scotland, \"EPSG:27700\") # set CRS\n\nlocations %&gt;% summary(\"geometry\") # epsg:27700\nscotland %&gt;% summary(\"geometry\") # epsg:4326\nplaces_geo_sf %&gt;% summary(\"geometry\") # epsg:4326\n\n\n## Digitise your own maps -- ArcGIS / QGIS"
  },
  {
    "objectID": "r-scripts/corr_reg.html",
    "href": "r-scripts/corr_reg.html",
    "title": "2. Correlation analysis",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"xlsx\")\n# install.packages(\"writexl\")\n\n\n# Load the packages you need\nlibrary(tidyverse) \nlibrary(readxl)\nlibrary(writexl)\n\n\n\n#################### CORRELATION ANALYSIS ###################\n\n# Introductory graph\nx &lt;- (1:30)     # range of x\nn &lt;- length(x)  # number of obs.\n\na = 3         # constant\nb = 0.3         # coefficient\nsd = 0.5        # standard deviation of the error term\nset.seed(1)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data0 &lt;- data.frame(x, y)\nsim_data0 &lt;- as_tibble(sim_data0)\nsim_data0 &lt;- sim_data0 %&gt;%\n  mutate(type = \"positive\")\n\na = 7.5         # constant\nb = 0         # coefficient\nsd = 3        # standard deviation of the error term\nset.seed(222)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data1 &lt;- data.frame(x, y)\nsim_data1 &lt;- as_tibble(sim_data1)\nsim_data1 &lt;- sim_data1 %&gt;%\n  mutate(type = \"no correlation\")\n\na = 10         \nb = -0.2         \nsd = 1.3\nset.seed(123)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data2 &lt;- data.frame(x, y)\nsim_data2 &lt;- sim_data2 %&gt;%\n  mutate(type = \"negative\")\nsim_data2 &lt;- as_tibble(sim_data2)\n\nsim_data &lt;- bind_rows(sim_data0, sim_data1, sim_data2) %&gt;% \n  mutate(type = factor(type, \n                       levels = c(\"positive\", \"negative\", \"no correlation\"), \n                       ordered = TRUE))\n\nsim_data %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +\n  facet_wrap(~ type, nrow = 1)\n\n\n#### Numerical variables ####\n\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\n\nview(data)\n\n# Describing the data first: age, weight\ndata %&gt;% \n  summarize(count_age = sum(!is.na(age)),   \n            mean_age = mean(age, na.rm = TRUE),\n            count_weight = sum(!is.na(weight)),   \n            mean_weight = mean(weight, na.rm = TRUE))\n\n# Let's express weight in kgs (instead of pounds)\ndata &lt;- data %&gt;%\n  mutate(weight_kg = 0.453592*weight)\n\n\n\n### Visually\n  # Scatterplot: Plotting the relationship between two variables\ndata %&gt;%  \n  ggplot() +\n  geom_point(aes(x = age, y = weight_kg)) \n\n  # focusing only on those younger than 20\ndata %&gt;%  \n  filter(age &lt;= 19) %&gt;%\n  ggplot() + \n    geom_point(mapping = aes(x = age, y = weight_kg)) + \n  scale_x_continuous(limits = c(9,19), breaks = seq(9,19, by = 1)) +\n  scale_y_continuous(limits = c(20,80), breaks = seq(20,80, by = 10))\n      # editing how the xaxis looks like\n\n  # focusing now on those aged 20 and older\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot() + \n  geom_point(mapping = aes(x = age, y = weight_kg)) \n\n\n## Pearson correlation coefficient\n  # it measures the direction and the strenght of the association between two variables\n  # it ranges between -1 to 1 being:\n    #  1 - perfect positive correlation\n    #     (strong - moderate - weak)\n    #  0 - no correlation\n    #     (strong - moderate - weak)\n    # -1 - perfect negative correlation\n\n  # it can be computed for numerical, ordinal and qualitative variables \n    # but employing different methods\n\n# compute it using age & weight (both numerical)\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  summarize(cor = cor(age, weight_kg, method = \"pearson\", use = \"complete.obs\")) \n    # -0.185\n    # \"use\" indicates how to handle missing values\n    # other options: \"everything\", \"all.obs\", \"complete.obs\", \"na.or.complete\", or \"pairwise.complete.obs\"\n\ndata %&gt;%  \n  filter(age &gt;= 20 & age &lt; 50) %&gt;%\n  group_by(sex) %&gt;%\n  summarize(cor=cor(age, weight_kg, method = \"pearson\", use = \"complete.obs\")) \n    # the negative link between age and weight is much stronger in women\n\n# visually:\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot() + \n    geom_point(aes(x = age, y = weight_kg)) +\n    facet_wrap(~ sex, nrow = 2)\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot() + \n  geom_point(aes(x = age, y = weight_kg, col = sex))\n\n\n## Illustrative graph - regression\nx &lt;- (1:30)     # range of x\nn &lt;- length(x)  # number of obs.\n\na = 0           # constant\nb = 6.2         # coefficient\nc = -0.2\nsd = 0.5        # standard deviation of the error term\nset.seed(2222)\ny = a + b*x + +c*x^2 + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data3 &lt;- data.frame(x, y)\nsim_data3 &lt;- as_tibble(sim_data3)\n\nsim_data3 %&gt;%  \n  summarize(cor = cor(x, y, method = \"pearson\", use = \"complete.obs\")) %&gt;%\n  mutate_if(is.numeric, round, 3)\n\nsim_data3 %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +  \n  annotate(x=25, label=\"r = -0.013\", y=41, colour=\"red\", geom = \"label\", size = 3)\n\n\n#### Regression analysis\n\n# Correlation analysis assesses the direction and strength of the relationship \n  # within a scale between 0-1 (whether two variables move together)\n\n# Regression analysis goes beyond and allows:\n  # (1) assessing the actual impact of X on Y: coefficient b\n  # (2) computing how much of the variation in Y is explained by X (or Xs): R-squared\n  # (3) allows controlling directly for the effect of other variables\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n# Regression analysis basically finds the best line to fit the data (OLS: Ordinary Least Squares)\n  # (the line minimising the sum of the deviations between the observed and predicted values)\n  #   (the deviations are squared in order to compare negative and positive deviations)\n  # \"lm\" refers to \"linear model\": Y = a + b*X   \n  # y is the variable we want to \"explain\" (weight; dependent variable)\n  # x is the explanatory variable (age)\n  # se refers to standard errors (the confidence intervals) - let's not report then for now\n\n# Estimate the regression line (intercept/slope)  \n  # R (or any other statistical software) does the job for you\n\nlibrary(modelr)\n\nlm(weight_kg ~ age, data = filter(data, age&gt;=20))\n  # where age is the explanatory variable (x) and weight_kg the dependent variable (y)\n\n# or with pipe\ndata %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .)\n\n  # interpret the results:\n    # y = a + b*x\n    # weight = 66.8 - 0.13*age\n    # intercept: if x = 0 -&gt; weight = 66.8\n    # slope: one-unit increase in X (age: 1 year) reduces Y (weight) by 0.13 units (kgs.)\n    # (always think about of units of measurement both in X and Y)\n\n\n# Predicted values: estimating what Y will be depending on Y\n\nreg &lt;- data %&gt;% \n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) \n\ndata &lt;- data %&gt;%\n  add_predictions(reg, var = \"pred\")\nview(data)\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) + # deactivating this line shows only the predicted values\n    geom_point(aes(x = age, y = pred), colour = \"red\", size = 1) \n\n# Residuals: difference between the observed and predicted values\n\nlabel &lt;- expression(e[i]) # illustrative graph\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) +\n    geom_segment(aes(x = 69, xend = 69, y = 73, yend = 58), colour = \"red\", size = 0.5) +\n    annotate(x=71, y=66, colour=\"red\", geom = \"label\", size = 3, label=label) +\n    geom_segment(aes(x = 66, xend = 66, y = 44.5, yend = 58.5), colour = \"red\", size = 0.5) +\n    annotate(x=68, y=51.5, colour=\"red\", geom = \"label\", size = 3, label=label) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n# Add the predicted values and the residuals to the dataframe\ndata &lt;- data %&gt;%\n  add_predictions(reg, var = \"pred\") %&gt;%\n  add_residuals(reg, var = \"resid\") \n\ndata %&gt;% \n  filter(age&gt;=20 & !is.na(weight_kg)) %&gt;%\n  select(age, weight_kg, pred, resid) \n\n# Ordinary Least Squares (OLS): finds the line that best fits the data\n  # estimates the line that minimises the sum of the deviations between the observed values (the dots) and the predicted values \n    # (the deviations are squared in order to avoid negative and positive deviations cancelling each other out)\n\ndata %&gt;%  # Illustrative graph only\n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) +\n    geom_point() +\n    geom_abline(intercept = 62, slope = -.04, colour = \"red\") +\n    geom_abline(intercept = 65, slope = -.10, colour = \"red\") +\n    geom_abline(intercept = 63, slope = -.03, colour = \"red\") +\n    geom_abline(intercept = 65, slope = -.04, colour = \"red\") +\n    geom_abline(intercept = 68, slope = -.17, colour = \"red\") +\n    geom_abline(intercept = 68, slope = -.14, colour = \"red\") +\n    geom_abline(intercept = 68, slope = -.10, colour = \"red\") +\n    geom_smooth(method = \"lm\", se = FALSE, colour = \"blue\", linewidth = 1)  \n\n\n## R-squared\n  # Fraction of the variation on Y that is explained by the model \n  # It ranges between 0 and 1\n    # Low R2 does not neccessarily mean that the model is useless\n    # It has to be interpreted based on expectations\n    # The coefficient \"b\" can still provide useful info about the effect of X on Y\n\n# Illustrative graph\nx &lt;- (1:30)     # range of x\nn &lt;- length(x)  # number of obs.\n\na = 0.2         # constant\nb = 0.3         # coefficient\nsd = 1        # standard deviation of the error term\nset.seed(11111)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data0 &lt;- data.frame(x, y)\nsim_data0 &lt;- as_tibble(sim_data0)\nsim_data0 &lt;- sim_data0 %&gt;%\n  mutate(type = \"high\")\n\na = 0.2         # constant\nb = 0.3         # coefficient\nsd = 2.5        # standard deviation of the error term\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data1 &lt;- data.frame(x, y)\nsim_data1 &lt;- as_tibble(sim_data1)\nsim_data1 &lt;- sim_data1 %&gt;%\n  mutate(type = \"low\")\n\nsim_data &lt;- bind_rows(sim_data0, sim_data1) # combine both # reorder the levels: pos, neg, no\n\nreg0 &lt;- sim_data %&gt;% \n  filter(type==\"high\") %&gt;%\n  lm(y ~ x, data = .)\n\nreg1 &lt;- sim_data %&gt;% \n  filter(type==\"low\") %&gt;%\n  lm(y ~ x, data = .)\n\nsummary(reg0)$r.squared \nsummary(reg1)$r.squared \n\nnew_labels &lt;- c(\"high\" = \"R-squared = 0.85\", \"low\" = \"R-squared = 0.44\")\nsim_data %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +\n  facet_wrap(~ type, nrow = 1, labeller = labeller(type = new_labels))\nsim_data %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ type, nrow = 1, labeller = labeller(type = new_labels)) # report the R2\n\n\n# Report R-squared\nreg &lt;- data %&gt;% \n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) # it does not automatically shows up\n\nsummary(reg)$r.squared # 0.03 - the model (age) explains 3 per cent of the variation in adult weight\n\n# install.packages(\"moderndive\")\nlibrary(moderndive)\n\ndata %&gt;% \n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) %&gt;%\n  get_regression_summaries() %&gt;%\n  select(nobs, r_squared)\n\ndata %&gt;% \n  filter(age&lt;20) %&gt;%\n  lm(weight_kg ~ age, data = .) %&gt;%\n  get_regression_summaries() %&gt;%\n  select(nobs, r_squared) # 0.695\n\n# Regression tables: how age and adult weight are related for men and women separately?\n\n# install.packages(\"modelsummary\")\nlibrary(modelsummary)\n\nreg_males &lt;- data %&gt;%\n  filter(age&gt;=20 & sex==\"male\") %&gt;%\n  lm(weight_kg ~ age, data = .)\n\nreg_females &lt;- data %&gt;%\n  filter(age&gt;=20 & sex==\"female\") %&gt;%\n  lm(weight_kg ~ age, data = .)\n\nmodelsummary(\n  list(\n    \"Males\" = reg_males,\n    \"Females\" = reg_females),\n  statistic = NULL,\n  gof_map = c(\"nobs\", \"r.squared\"),\n  fmt = 2) \n\n    # obviously these results are contingent on the data we are using:\n      # smaller numer of female observations\n      # also, are men and women comparable in this setting (who ended up in prison)?\n        # controls? ...\n      # biological vs social? --&gt; single / married women? diets in prison?\n    # non-linearities?\n    # outliers\n\n\n# Visually:\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg, color = sex)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n### Qualitative variables \n  # categorical (or ordinal) variable can be used in regression analysis as dummy variables (0/1)\n    # 1 category acts as a \"reference category\"\n    # so the coefficient is interpreted \"against\" that category\n    # if you have n categories, you need (n-1) dummies\n    # i.e. sex has two categories: male/female, so you just need 1 dummy\n      # either being male or being female \n    # no need to create dummy variables to include categorical info (R does it for you)\n\ndata %&gt;% \n  filter(age&gt;=20) %&gt;%\n  group_by(sex) %&gt;%\n  summarize(\n    mean_height = mean(height, na.rm = TRUE))\n\ndata &lt;- data %&gt;% \n  mutate(male = ifelse(sex==\"male\", 1, 0)) # what to do with missing values, apparently still missing, check\ndata %&gt;%\n  count(male)\n\ndata %&gt;% \n  subset(select=c(forename, sex, male)) %&gt;% # check what you have done\n  print(n = 20) \n\ndata %&gt;%\n  filter(age &gt;= 20) %&gt;%\n  lm(height ~ male, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate) # men are 12.5 centimeters taller than women\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(aes(x = male, y = height)) + # visually\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_hline(yintercept = 156.5729, color = \"red\", linetype = 2) +\n  geom_hline(yintercept = 169.0388, color = \"blue\", linetype = 2) +\n  scale_x_continuous(breaks = seq(0, 1, 1))\n\n# doing the same with country of birth\ndata %&gt;%\n  filter(age &gt;= 20 & sex==\"male\") %&gt;%\n  lm(height ~ countryb, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n  # no need to create the dummies yourself but you lose control of the reference category\n\n# creating the dummies yourself\ndata &lt;- data %&gt;%  \n  mutate(england = ifelse(countryb==\"england\", 1, 0)) %&gt;%\n  mutate(ireland = ifelse(countryb==\"ireland\", 1, 0)) %&gt;%\n  mutate(scotland = ifelse(countryb==\"scotland\", 1, 0)) %&gt;%\n  mutate(overseas = ifelse(countryb==\"overseas\", 1, 0))\n\ndata %&gt;% \n  subset(select=c(forename, countryb, england, ireland, scotland, overseas)) %&gt;%\n  print(n = 20) \n\ndata %&gt;%\n  filter(age &gt;= 20 & sex==\"male\") %&gt;%\n  lm(height ~ england + ireland + overseas, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n\n\n### Qualitative variables as dependent variables (Y)\ndata %&gt;%\n  count(lit_adj)\n\ndata %&gt;%\n  group_by(lit_adj) %&gt;%\n  summarize(n = n()) %&gt;%\n  mutate(freq = n/sum(n))\n\ndata &lt;- data %&gt;%\n  mutate(write = ifelse(lit_adj==\"write\", 1, 0)) %&gt;%\n  mutate(male = ifelse(sex==\"male\", 1, 0))\n\ndata %&gt;%\n  count(write)\n\ndata %&gt;% summarize(\n  mean_write = mean(write, na.rm = TRUE))\n\ndata %&gt;%\n  lm(write ~ male, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n  # being male (going from 0, female, to 1, male) is associated with an increase in literacy of 0.07 \n    # (or an increase of 7 percentile points if measured as percentages)\n\n\n\n## Comparing the relative importance of different variables\n\nreg_height &lt;- data %&gt;% \n  mutate(height = 30.48*feet+2.54*inches) %&gt;% # in case not previously created\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ height, data = .) \n\nreg_sex &lt;- data %&gt;% \n  mutate(male = if_else(sex==\"male\", 1, 0)) %&gt;% # in case not created before\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ male, data = .) \n\nmodelsummary(\n  list(\n    \"M1\" = reg_height,\n    \"M2\" = reg_sex),\n  statistic = NULL,\n  gof_map = c(\"nobs\", \"r.squared\"),\n  fmt = 3)\n\n\ndata %&gt;% \n  mutate(height = 30.48*feet+2.54*inches) %&gt;% \n  mutate(height_mts = height/100) %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ height_mts, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n  # changing the units of measurement does not change the results (only their scale) \n\n\n## Time as explanatory variable\n\ndata %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  filter(age &gt;= 20) %&gt;%\n  lm(height ~ year_birth, data = .) %&gt;%\n  ggplot(aes(y = height, x = year_birth)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    annotate(x=1780, y=167, label=\"Y = 118.1 + 0.026*X\", colour=\"red\", geom = \"label\", size = 3)\n\n# individual years as explanatory variables\ndata %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  mutate(year_birth = as_factor(year_birth)) %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  lm(height ~ year_birth, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate) %&gt;% \n  print(n = Inf)\n\n# grouping years into age groups\ndata &lt;- data %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  mutate(year_birth_10 = cut(year_birth, breaks =  seq(1775, 1885, 10)))\ndata %&gt;%\n  count(year_birth_10)\n\ndata %&gt;%\n  filter(age&gt;=20 & year_birth&gt;1795) %&gt;%\n  lm(height ~ year_birth_10, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)"
  },
  {
    "objectID": "r-scripts/uncert.html",
    "href": "r-scripts/uncert.html",
    "title": "3. Uncertainty (chance)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"xlsx\")\n# install.packages(\"writexl\")\n\n\n# Load the packages you need\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(modelr)\n\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\ndata &lt;- data %&gt;%\n  mutate(weight_kg = 0.453592*weight)\n\n\n#################### Dealing with uncertainty ###################\n\n# What we observe is influenced by chance\n# What would have happened if we would have observed a different set of records (sample)\n\n# We draw conclusion about the characteristics of the population\n  # based on a sample of observations\n  # this sample statistic however will deviate from the true value\n  # --&gt; sampling error, which depends on:\n    # the variability within the population (st. dev.)\n    # the number of observations in the sample\n  # --&gt; we use the standard error to compute:\n    # confidence intervals\n      # it will cover the true value with certain probability\n    # testing hypothesis:\n      # assume a hypothesis to be true\n      # assess the probability (p-value) of what is observed\n        # based on the previous assumption\n          # the probability that the observed outcome would\n            # have happened by chance\n\n\n\n### Standard error\ndata %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & countryb!=\"\") %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    sd = sd(height, na.rm=TRUE),\n    se = sd/sqrt(obs)) %&gt;%\n  mutate_if(is.numeric, round, 2)\n  # larger sample size, lower standard errors\n\n### Confidence intervals\n\nci &lt;- data %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    st.dev = sd(height, na.rm=TRUE),\n    st.error = sd/sqrt(obs), \n    lb_ci = t.test(height, conf.level = 0.95)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.95)$conf.int[2]) %&gt;%  # CI upper bound\n  mutate_if(is.numeric, round, 1) # add 1 decimal place\nci\n\n# Visually\nci %&gt;%  \n  ggplot(aes(x = countryb, y = mean)) +\n    geom_point() +\n    geom_errorbar(aes(x=countryb, ymin=lb_ci, ymax= ub_ci), width = 0.1)\n\n  # using sex instead\nci_sex &lt;- data %&gt;%\n  filter(age&gt;=20 & countryb==\"england\") %&gt;%\n  group_by(sex) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    st.dev = sd(height, na.rm=TRUE),\n    st.error = sd/sqrt(obs), \n    lb_ci = t.test(height, conf.level = 0.95)$conf.int[1],   # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.95)$conf.int[2])   # CI upper bound\n\nci_sex %&gt;% ggplot(aes(x = sex, y = mean)) +\n  geom_point() +\n  geom_errorbar(aes(x=sex, ymin=lb_ci, ymax= ub_ci), width = 0.3)\n\n\n\n# Confidence level\n\nci99 &lt;- data %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    lb_ci = t.test(height, conf.level = 0.99)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.99)$conf.int[2]) %&gt;%  # CI upper bound\n  mutate(conf_level = \"Conf. level = 99\")\nci99\nci90 &lt;- data %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    lb_ci = t.test(height, conf.level = 0.90)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.90)$conf.int[2]) %&gt;%  # CI upper bound\n  mutate(conf_level = \"Conf. level = 90\")\nci90\nci90_99 &lt;- bind_rows(ci99, ci90)\nci90_99\nci90_99 %&gt;%  \n  ggplot(aes(x = countryb, y = mean)) +\n  geom_point() +\n  geom_errorbar(aes(x=countryb, ymin=lb_ci, ymax= ub_ci), width = 0.1) +\n  facet_wrap(~ conf_level, nrow = 1)\n\n\n# Apply the same idea to changes over time\ndata %&gt;%\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50 & year&gt;1846) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    mean = mean(weight_kg, na.rm = TRUE),\n    lb_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[2]) %&gt;%  # CI upper bound\n  ggplot(aes(x = year, y = mean)) +\n    geom_point() +\n    geom_errorbar(aes(x = year, ymin = lb_ci, ymax = ub_ci), width = 0.1)\n\n# this solution is slightly different\ndata %&gt;%\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50 & year&gt;1846) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    mean = mean(weight_kg, na.rm = TRUE),\n    lb_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[2]) %&gt;%  # CI upper bound\n  ggplot(aes(x = year, y = mean)) +\n    geom_smooth(se = TRUE) +\n    geom_point() +\n    geom_errorbar(aes(x = year, ymin = lb_ci, ymax = ub_ci), width = 0.1)\n\n\n### Confidence intervals in correlation and regression analysis\n  # the results also come from a particular sample (what if...)\n\n# Correlation\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  group_by(sex) %&gt;%\n  summarise(cor = cor(age, weight_kg, method = \"pearson\", use = \"complete.obs\"),\n            cor_ci_lb = cor.test(age, weight_kg)$conf.int[1],\n            cor_ci_ub = cor.test(age, weight_kg)$conf.int[2]) \n\n# Regression\nlibrary(modelr)\nlibrary(moderndive)\n\ndata %&gt;% \n  mutate(weight_kg = 0.453592*weight) %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate, std_error, lower_ci, upper_ci) %&gt;%\n  mutate_if(is.numeric, round, 2)\n\n# visually \ndata %&gt;% \n  mutate(weight_kg = 0.453592*weight) %&gt;%\n  filter(age&gt;=20) %&gt;%\n  ggplot(aes(y = weight_kg, x = age)) +\n    geom_point() +\n    geom_smooth(method = \"lm\") # se = TRUE\n\n\n### Hypothesis testing, t-tests, and p-values\n  # asking concrete questions and let the statistics reject the hypotheses or not\n    # null hypothesis (H0): reflects a conservative attitude towards the potential findings \n      # by specifying the negative form of a proposition. \n        # there is not difference between two means \n        # that there is no relationship between two variables. \n    # research (alternative) hypothesis (H1) specifies the opposite\n\n  # the p-value assess the probability that the observed outcome would be present \n    # if the null hypothesis were true \n    # (the probability that the observed outcome would have happened by chance). \n      # if the probability of having observed that outcome is very low, \n        # we should therefore consider rejecting the null hypothesis, \n          # that there is no difference between the means or \n          # that there is no correlation between two variables). \n\n  # as with confidence intervals, we reject hypotheses with a certain probability (**confidence level**). \n    # in order to decide whether we reject the null or not, \n      # the *p-value* is compared against α, the **significance level**, \n        # which basically mirrors the confidence level (α = 100 minus the confidence level):\n      # -- if p-value is large (&gt;α), the data supports the null hypothesis\n      # -- if p-value is small (&lt;α), the evidence is against the null hypothesis\n\n# Example\ndata %&gt;%\n  filter(countryb==\"scotland\" & sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE)) # average height\n\n# One-sample test\n  # Scottish males are 171 cms on average (from other source)\n  # are our prisoners similar?\n\ndata %&gt;%\n  filter(countryb==\"scotland\" & sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%  \n  t.test(height ~ 1, mu = 171, data = .,\n         conf.level = 0.95)\n\n# Two-sample test: comparing groups\ndata %&gt;%\n  filter(countryb==\"scotland\" | countryb==\"ireland\") %&gt;%  \n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  t.test(height ~ countryb, data = .,\n         conf.level = 0.95)\n\n\n\n## Hypothesis testing in regression analysis\n\nreg_m &lt;- data %&gt;% \n  filter(age&gt;=20 & sex==\"male\") %&gt;%\n  lm(weight_kg ~ age, data = .) \n\nreg_f &lt;- data %&gt;% \n  filter(age&gt;=20 & sex==\"female\") %&gt;%\n  lm(weight_kg ~ age, data = .) \n\nmodelsummary(\n  list(\n    \"Males\" = reg_m,\n    \"Females\" = reg_f),\n  statistic = c(\"s.e. = {std.error}\", \"p-value = {p.value}\"),\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\"),\n  fmt = 3,\n  coef_omit = \"Intercept\",\n  note = \"For simplicity, the intercept is not reported\")"
  },
  {
    "objectID": "r-scripts/desc_qual.html",
    "href": "r-scripts/desc_qual.html",
    "title": "1.1. Qualitative variables",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n#### Quantification in History ####\n\n# These scripts provide the code that help exploring the \"Paisley dataset\".\n# They are intended as an introductory guide to how quantification works\n# They can also be easily replicated\n\n# Notice that the symbol \"#\" allows creating \"comments\" (in green) to the code\n\n\n## Getting ready (preparatory commands)\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# Set working directory\ngetwd() # Provides the current working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\")\n  # Sets the working directory(THIS IN MINE; ***CHANGE IT TO YOURS***)\n\n  # it is sometimes difficult to type the correct directory path (the one above is mine)\n  # setting it manually through the menu helps (Session/Set working directory/Choose directory)\n  # copy and paste it into the script later, so you don't need to do it again\n\n  # once the working directory is set\n  # we use \"relative paths\" within this environment\n  # vreate a folder structure that makes working on this project easy\n    # data, results, etc.\n\n\n### Install packages (if needed; only once)\n# install.packages(\"tidyverse\")\n# install.packages(\"knitr\")\n# install.packages(\"xlsx\")\n# install.packages(\"writexl\")\n\n    # R works using different tools that are contained\n    # in different packages\n    # they need to be installed (only once)\n    # and open before using (each session)\n\n### Load the packages you need (you will need to do it every session)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(readxl)\n\n# Import data\ndata &lt;- read_excel(\"data/paisley_data.xls\")\n  # notice that we are using a \"relative path\"  \n  # if the data is in the working directory, you don't need the path (just the file name)\n  # if the data is somewhere else (not within the project environment), you need to use the absolute path\n    # i.e.-- paisley_data &lt;- read_excel(\"~/FRAN/Teaching/QM Research School/Datasets/paisley_data.xls\")\n    # going back in the folder structure: paisley_data &lt;- read_excel(\"../Datasets/paisley_data.xls\")\n  \n  # notice the \"&lt;-\" \n    # it creates a (temporary) object that is now in the \"environment\" we are working with\n      # We could properly save it (as a separate file) if we wished \n    # you can load as many \"objects\" as you wish (and they can be treated separately)\n\n# Inspecting the data\ndata       # A peek at the data\n    # Notice that some values are missing (NA; not available; the source did not provide info)\n    # It also indicates whether categorical (character) or numerical (double...)\n    # Types of variables:\n        # numeric: dbl (double; there are other types: integer...)\n        # string: chr (character)\n        # factor, ...\n\nview(data) # The whole dataset\nhead(data) # the first observations\ntail(data) # the last observations\n\ntbl_vars(data) # list the variables (fields) in the dataset\n\n## We will be using the pipe (%&gt;%) though\n\ndata %&gt;%  \n  head(15)\n\n## NO NEED TO LEARN THE COMMANDS BY HEART\n  # copy and paste from the templates\n  # you will get better with practice\n  # google is always your friend (r table frequency table)\n  # we now focus on make the commands work but we will later focus on interpreting the results\n  # (you will be able to work on the code on your own)\n  # in any case, we will only be scraping the surface (a brave new world out there)\n\n\n#################### DESCRIPTIVE STATISTICS (1 variable) ###################\n\n############## Categorical (qualitative) variables ############\n# sex, literacy, employed ...\n# each of these variables can exhibit certain values (categories)\n  # e.g. sex: male / female\n\n#### Frequency table (tabulate)\n  # number of observations falling in each category \n\n  # number of males and females\n\ndata %&gt;%  # returns a table\n  count(sex)\n  # the pipe (also |&gt;) meaning \"and then\"\n  # it takes the output of a line of code and uses it as input to the next line\n\ndata %&gt;%  # returns a table\n  count(employed)\n  # notice that is also reports the number of missing values\n\ndata %&gt;%  # returns a table\n  count(occup) %&gt;%\n  print(n = 20) # display 20 rows\n\ndata %&gt;%  # returns a table\n  count(occup) %&gt;%\n  print(n = Inf) # display all rows\n\ndata %&gt;%  \n  count(occup, sort = TRUE) # present the data in order\n                            # important to quickly identify the most important categories\n\n## Absolute and relative frequencies\ndata %&gt;% \n  count(occup, sort = TRUE) %&gt;%              \n  mutate(proportion = n/sum(n))  # adding relative frequency \n\ndata %&gt;% \n  count(occup, sort = TRUE) %&gt;%              \n  mutate(prop = n/sum(n)) %&gt;% # fractions\n  mutate(perc = 100*n/sum(n)) # percentages \n\n# or\ndata %&gt;% \n  group_by(sex) %&gt;%       # dimension we focus on\n  summarize(n = n()) %&gt;%  # reporting number of observations in each group\n  mutate(rel_freq = n/sum(n)) # creating a new variable with the relative frequency \n\n  # notice that the code may look tricky but is easily replicable\n  # with literacy\ndata %&gt;% \n  group_by(literacy) %&gt;%\n  summarize(n = n()) %&gt;%\n  mutate(freq = n/sum(n))\n\n# Adding another row with the totals:\nlibrary(janitor)\n\ndata %&gt;%  \n  count(countryb, sort = TRUE) %&gt;%              \n  mutate(perc = 100*n/sum(n)) %&gt;%\n  adorn_totals(\"row\") %&gt;%\n  as_tibble()\n\n\n\n### Focusing on subsamples (smaller groups) \n  # filter()\ndata %&gt;% \n  filter(sex == \"male\") %&gt;%   \n  count(literacy, sort = TRUE) %&gt;%\n  mutate(prop = n/sum(n))\n  \n  # \"filter\" restricts the analysis to those observations fulfilling that condition\n  # filter() allows for complex selections using different operators:\n    # &gt;, &lt;, ==, != (distinct)\n    # Notice also that you can create more complex conditions using:\n      # and: &\n      # or: |\n      # except: - \n\ndata %&gt;% \n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) %&gt;%   \n  count(literacy, sort = TRUE) %&gt;%              \n  mutate(prop = n/sum(n)) \n\ndata %&gt;% \n  filter(countryb==\"ireland\" | countryb==\"scotland\") %&gt;%\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) %&gt;%   \n  count(literacy, sort = TRUE) %&gt;%              \n  mutate(prop = n/sum(n)) %&gt;%\n  mutate(prop = round(prop, 2)) # reporting only up two decimals\n\n# This allows paying attention to what it is both common and rare\ndata %&gt;% \n  filter(countryb==\"ireland\" | countryb==\"scotland\") %&gt;%\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) %&gt;%   \n  filter(literacy==\"superior education\") %&gt;%\n  select(year, forename, surname, age, born, countryb, reside, occup)\n\n\n### Create an output that you can use later\n  # the output is not saved unless it is assigned to an object\n  # which can then be exported (saved) into different formats later\n\ntable1 &lt;- data %&gt;% \n  filter(sex == \"male\") %&gt;%   \n  group_by(literacy) %&gt;%      \n  summarize(n = n()) %&gt;%\n  mutate(freq = n/sum(n)) \n\n# Export the output (object) into excel \nlibrary(writexl)\n\nwrite_xlsx(occ, \"output/table1.xlsx\")\n  # notice that we are saving this \"object\" as an excel file in the folder \"output\"\n\n\n\n###### Plotting frequencies (graph bar)\n  # ggplot\n  # use \"+\" to add features to the graph\n\n# sex\nggplot(data = data) +       \n  geom_bar(aes(x = sex))\n    # \"aes\" goes for aesthetics\n\n  # or\ndata %&gt;% \n  ggplot() + \n    geom_bar(aes(x = sex))\n\n# literacy\ndata %&gt;% \n  ggplot() + \n    geom_bar(aes(x = literacy))\n\n# horizontal to facilitate reading the categories\ndata %&gt;% \n  ggplot() + \n    geom_bar(aes(x = literacy)) +\n    coord_flip()\n\n# narrowing down the analysis  \ndata %&gt;% \n  filter(countryb == \"scotland\" | countryb == \"ireland\") %&gt;%\n  filter(sex == \"male\" & age&gt;18) %&gt;%\n  ggplot() + \n    geom_bar(mapping = aes(x = literacy)) +\n    coord_flip()\n\n\n# proportions (instead of counts)\ndata %&gt;% \n  count(literacy, sort = TRUE) %&gt;%              \n  mutate(prop = n/sum(n)) %&gt;%\n  ggplot(aes(x = literacy, y = prop)) + \n    geom_col() + # instead of geom_bar()\n    coord_flip()\n  \n    # geom_col() is a more general way of depicting graph bars\n    # we explicitly indicate what we want to depict (prop in this case)\n\n# editing the graph\ng1 &lt;- data %&gt;% \n  count(literacy, sort = TRUE) %&gt;%              \n  mutate(prop = n/sum(n)) %&gt;%\n  ggplot(mapping = aes(x = literacy, y = prop)) + \n    geom_col() + \n    coord_flip() + \n    ylab(\"Proportion\") + \n    xlab(\"Literacy\")\n  # or all together: \n  # labs(title =\"\", subtitle = \"\", x = \"\", etc etc)\n  # many options for editing graphs... \n\ng1  \n# save the graph\ng1 %&gt;%\n  ggsave(\"output/lit.png\", dpi = 320)\n\n?ggsave\n?ggplot\n\n\n### Re-categorising qualitative information \n# typos, aggregating categories, etc.\n\ndata %&gt;%  \n  count(occup) %&gt;%              \n  mutate(prop = n/sum(n)) \n\n# recoding\ndata &lt;- data %&gt;%\n  mutate(occup_adj = recode(occup, \n                            \"blacksmith\" = \"black smith\", \n                            \"block print\" = \"block printer\")\n         )\n\nView(data)\n\ndata %&gt;% \n  filter(occup_adj==\"black smith\" | occup_adj==\"block printer\") %&gt;%\n  select(occup, occup_adj)\n\ndata %&gt;%  \n  count(literacy)\n\n  # checking what we have done\n\n# another example with literacy: iliterate, read, write\ndata &lt;- data %&gt;%\n  mutate(lit_adj = recode(literacy, \n                          \"superior education\" = \"write\", \n                          \"read well\" = \"read\", \n                          \"read tolerably\" = \"read\", \n                          \"read a little\" = \"read\",\n                          \"read & write well\" = \"write\", \n                          \"read & write tolerably\" = \"write\", \n                          \"read & write a little\" = \"read\", \n                          \"cannot write\" = \"read\")\n  )\n    # no need to recode \"illiterate\" because it is the same category\n\ndata %&gt;% \n  subset(select=c(forename, surname, literacy, lit_adj))\n\ndata %&gt;%\n  count(lit_adj)\n\n# removing leading and trailing spaces\ndata &lt;- data %&gt;% \n  mutate(occup_adj = str_trim(occup_adj))\n\n# ranking \"qualitative\" information\ndata %&gt;%  \n  count(literacy) %&gt;%              \n  mutate(prop = n/sum(n)) \n  # listed in alphabetical order (not always informative enough)\n\n\n# factor variables: ranking categories\ndata &lt;- data %&gt;% \n  mutate(lit_rank = factor(literacy, \n                           levels = c(\"illiterate\", \n                                      \"read a little\", \n                                      \"read tolerably\", \n                                      \"read well\", \"cannot write\", \n                                      \"read & write a little\", \n                                      \"read & write tolerably\", \n                                      \"read & write well\", \n                                      \"superior education\"), \n                           ordered = TRUE))\n\ndata %&gt;%\n  count(lit_rank)\n  \n  # or visually:\ndata %&gt;% \n  filter(!is.na(lit_rank)) %&gt;% # exclude missing values\n  ggplot() +\n    geom_bar(aes(x = lit_rank)) +\n    coord_flip()  \n\n# re-categorising more complicate patterns\n\ndata %&gt;%\n  count(marks)\n\n# install.packages(\"stringr\")\nlibrary(stringr)\n\ndata &lt;- data %&gt;%\n  mutate(marks_adj = case_when(\n    str_detect(marks, \"scar\") ~ \"scar\",\n    str_detect(marks, \"cut\") ~ \"cut\", \n    str_detect(marks, \"burn\") ~ \"burn\",  \n    str_detect(marks, \"blind\") ~ \"blind\",      \n    str_detect(marks, \"no mark\") ~ \"none\",\n    str_detect(marks, \"nomark\") ~ \"none\",    \n    str_detect(marks, \"mark\") ~ \"mark\",\n    is.na(marks) ~ NA,\n    TRUE ~ \"other\"))\n\n# checking what you have done\ndata %&gt;%\n  select(marks, marks_adj)\n\ndata %&gt;% \n  count(marks_adj)\n\ndata %&gt;% \n  filter(marks_adj==\"cut\") %&gt;% \n  count(marks) %&gt;% \n  print(n = Inf)\n\n\n# Regular expressions\n\n\n##### Save the (modified) data ####\n# The data has somewhat change: correct typos, add new variables...\n# We can save the new dataset as a R file, so we can come back to that later\n\nwrite_rds(data, \"paisley_v2.rds\")\n\n\n\ndata2 &lt;- read_rds(\"paisley_v2.rds\")"
  },
  {
    "objectID": "r-scripts/desc_num.html",
    "href": "r-scripts/desc_num.html",
    "title": "1.2. Numerical variables",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# upload basic packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\n# import data\ndata &lt;- read_excel(\"data/paisley_data.xlsx\")\n\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\n\n############## Numerical variables ############\n\n# age, weight, height, ...\ndata\n\n# Reporting frequencies is not useful when there are many values\ndata %&gt;%  \n  count(age) %&gt;%              \n  mutate(perc = 100*n/sum(n)) %&gt;%\n  print(n = Inf) # display all rows\n\npaisley_data %&gt;% \n  count(age) %&gt;%\n  print(n = Inf) # display all rows\n\n# Potential solution: Create class intervals (group values into bins)\ndata &lt;- data %&gt;% \n  mutate(age_class = cut(age, breaks = 5)) # Creates groups of equal size\n\ndata %&gt;% \n  count(age_class) %&gt;%\n  print(n = Inf) # display all rows\n\n# or setting when the breaks happen yourself\ndata %&gt;% \n  mutate(age_class = cut(age, breaks = c(0, 14, 19, 50, Inf))) %&gt;%\n  count(age_class) %&gt;%\n  mutate(perc = 100*n/sum(n)) # report relative frequencies as well\n\n# or even assign \"labels\" to those groups\ndata %&gt;% \n  mutate(age_class = cut(age, breaks = c(0, 14, 19, 50, Inf), \n                         labels = c(\"Children\", \"Youngters\", \"Adults\", \"Elderly\"))) %&gt;%\n  count(age_class)\n  \n\n\n## Histogram (visual representation of the distribution)\ndata %&gt;%   \n  ggplot(mapping = aes(x = age)) +\n    geom_histogram(binwidth = 5)\n\n# Play around with the width of the bin: accuracy vs noise\n  # ggplot uses the + operator (to add layers)\n  # 3 key aspects: data, aesthetics and type\n\ng1 &lt;- data %&gt;%   \n  ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 1)\n\ng2 &lt;- data %&gt;%   \n  ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 10)\n\n# put the graphs together\nlibrary(patchwork)\ng1 + g2\n\nggsave(\"output/hist_age.pdf\") # save plot\n\n# Focusing on particular subsamples\ndata %&gt;%   \n  filter(sex == \"male\") %&gt;% \n  ggplot() +\n  geom_histogram(mapping = aes(x = age), binwidth = 1)\n\n# Or by group\ndata %&gt;%   \n  ggplot() +\n  geom_histogram(mapping = aes(x = age), binwidth = 5) +\n  facet_wrap(~ sex, nrow = 1)\n\n### Summarise: report summary statistics\n\n# count: n(), mean, median, min, max, sd, IQR, min, quantile(x, 0.25)...\n# first, nth(x, 2), last\n# !is.na(x), n_distinct(x)\n\n# average age\ndata %&gt;% \n  summarize(mean_age = mean(age, na.rm = TRUE))\n  # na.rm removes missing values from the computations\n  # stands for \"NA remove\"\n  # if not included it results in NA because it cannot be computed\n  # instead of yielding missing outputs (check removing that condition)\n\n# first assign a name and then define what you want to do using functions\n\ndata %&gt;% \n  summarize(\n    count = sum(!is.na(age)),   # \"!is.na\" indicates \"is not na\" (not available=missing value)\n    mean_age = mean(age, na.rm = TRUE), # the comma allows for asking for more statistics\n    min_age = min(age, na.rm = TRUE),\n    max_age = max(age, na.rm = TRUE),\n    p25_age = quantile(age, 0.25, na.rm = TRUE),\n    p75_age = quantile(age, 0.75, na.rm = TRUE)  \n  )\n\n\n# Illustration:\ndata %&gt;%   \n  ggplot(mapping = aes(x = age)) +\n    geom_histogram(colour = \"grey70\", alpha = 0.2) +\n    geom_vline(aes(xintercept=29.6)) +\n    geom_vline(aes(xintercept=27)) +\n    geom_vline(aes(xintercept=9)) +  \n    geom_vline(aes(xintercept=89)) +\n    geom_vline(aes(xintercept=17)) +  \n    geom_vline(aes(xintercept=47)) + \n    geom_segment(aes(x = 29.6-12.1, y = 1, xend=29.6+12.1, yend = 1), color = \"red\", linewidth = 2) + \n    annotate(x=29.6, label=\"Mean\", y=142, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=27, label=\"Median\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=9, label=\"Min\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=89, label=\"Max\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=17, label=\"p10\", y=150, colour=\"red\", geom = \"label\", size = 3) +  \n    annotate(x=47, label=\"p90\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=29.6, label=\"Standard deviation\", y=5, colour=\"red\", geom = \"label\", size = 3) +\n    scale_x_continuous(name = \"Age\", breaks = seq(0,90,10)) +\n    scale_y_continuous(name = \"Frequency\")\n\n\n# Shape of the distribution: differences\n\nlibrary(patchwork)\np1 &lt;- data %&gt;%  \n  filter(age&gt;15) %&gt;%\n  ggplot(mapping = aes(x = weight)) +\n  geom_histogram()\n\np2 &lt;- data %&gt;%\n  filter(age&gt;15) %&gt;%\n  ggplot(mapping = aes(x = age)) +\n  geom_histogram()\n\np1 + p2\n\n# Summarise by group(s): \ndata %&gt;% \n  group_by(countryb) %&gt;%\n  summarize(\n    count = sum(!is.na(age)),\n    mean_age = mean(age, na.rm = TRUE),\n    min_age = min(age, na.rm = TRUE),\n    max_age = max(age, na.rm = TRUE)\n  )\n\n# by several groups\npaisley %&gt;% \n  group_by(sex, countryb) %&gt;%\n  summarize(\n    count = sum(!is.na(age)), \n    mean_age = mean(age, na.rm = TRUE),\n    median_age = median(age, na.rm = TRUE)\n  )\n\n# store the results in a new object\nby_sex_countryb &lt;- paisley_data %&gt;%   \n  group_by(sex, countryb) %&gt;%\n  summarize(count = sum(!is.na(age)), \n            mean_age = mean(age, na.rm = TRUE),\n            median_age = median(age, na.rm = TRUE)\n  )\n\nview(by_sex_countryb)\n\n# Export to excel (note that is appended as a differet sheet in the previous file)\nlibrary(\"writexl\")\nwritexl(by_sex_countryb, path = \"table2.xlsx\")\n\n\n### Modifying and creating numerical variables\n\n# Inspect info on heights: feet, inches\n\ng1 &lt;- data %&gt;%   \n  ggplot(aes(x = feet)) +\n  geom_histogram()\n\ng2 &lt;- data %&gt;%   \n  ggplot(aes(x = inches)) +\n  geom_histogram()\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\ng1 + g2 # put the graphs together\n\ndata %&gt;%   \n  ggplot(aes(x = age)) +\n  geom_histogram()\n\n\n# something is going on\n\ndata %&gt;% \n  subset(age&gt;100) # Identify this case\n\n## Correct numerical value (typo)\ndata &lt;- data %&gt;%\n  mutate(age = replace(age, age == 160, 16))\n\n\n# check you dit it right\ndata %&gt;% \n  subset(forename==\"FRANCIS\" & surname==\"GILFILLAN\", \n         select=c(forename, surname, age))\n\n\n\n\n# Create nex variable (height) combining both variables (feet & inches)\ndata &lt;- data %&gt;%\n  mutate(height = 30.48*feet+2.54*inches)\n\nview(data)\n# a new variable has been added (last column on the right)\n\n# visualise new variable\ndata %&gt;%  \n  ggplot() +\n  geom_histogram(mapping = aes(x = height), binwidth = 5)\n\n\n# Group a numerical variable into different bins\ndata &lt;- data %&gt;%\n  mutate(height_bins = case_when(height &lt; 145 ~ 'low',\n                                 height &gt;=145 & height&lt; 175 ~ 'med',\n                                 height &gt;=175 ~ 'high'))\n# check\ndata %&gt;% \n  group_by(height_bins) %&gt;%\n  summarize(n = n()) %&gt;%\n  mutate(freq = n/sum(n))\n\n\n### Dummy variables \n\n# Allow quantifying qualitative variables\n\ndata %&gt;%\n  count(literacy)\n\ndata &lt;- data %&gt;%\n  mutate(lit_adj = recode(literacy, \n                          \"superior education\" = \"write\", \n                          \"read well\" = \"read\", \n                          \"read tolerably\" = \"read\", \n                          \"read a little\" = \"read\",\n                          \"read & write well\" = \"write\", \n                          \"read & write tolerably\" = \"write\", \n                          \"read & write a little\" = \"read\", \n                          \"cannot write\" = \"read\")\n  )\n\ndata %&gt;%\n  count(countryb)\n\n\ndata &lt;- data %&gt;%\n  mutate(scotland = ifelse(countryb==\"scotland\", 1, 0)) \n\ndata %&gt;%\n  select(countryb, scotland) \n\n# summary statistics\ndata %&gt;% \n  summarize(\n    obs = sum(!is.na(scotland)),   \n    mean = mean(scotland, na.rm = TRUE), \n    sd = sd(scotland, na.rm = TRUE), \n    min = min(scotland, na.rm = TRUE),\n    max = max(scotland, na.rm = TRUE)) %&gt;%\n  mutate_if(is.numeric, round, 3)\n\n  # the average of a dummy variable reflects the fraction (proportion) of \n    # observations belonging to that category \n  # it can also be interpreted as the probability of belonging to that group \n    # if an observation were selected at random.\n\n\n\n\n\n\n\n\n### More on graphing with ggplot\n# 3 key aspects: data, aesthetics (mapping) and type\npaisley_data %&gt;%  \n  filter(age &gt;= 18) %&gt;%\n  group_by(countryb) %&gt;%\n  summarize(mean_height = mean(height, na.rm = TRUE)) %&gt;%\n  ggplot(mapping = aes(x = countryb, y = mean_height)) +\n  geom_bar()\n# you can run chunks of code to see what the do so far\n# and identify where the problem lies\n# always consider what the unit of analysis is (when different levels)"
  },
  {
    "objectID": "get-r.html",
    "href": "get-r.html",
    "title": "Intro to R",
    "section": "",
    "text": "Implementing quantitative or computational analyses to historical (or any other) data requires some sort of statistical software. We will rely on R, a open-source free statistical software widely used by practitioners in many different fields both inside and outside academia. Although it is possible to work directly in R, using an integrated interface such as RStudio makes things much easier. RStudio basically integrates a text editor with the R console, so you can write, run and see the results of your analyses more easily.\nWe will be therefore download and install both software (R and RStudio) in your computer. Here is a link to access them (the version you install depends on whether you are using Windows or Mac)."
  },
  {
    "objectID": "get-r.html#r-interface",
    "href": "get-r.html#r-interface",
    "title": "Intro to R",
    "section": "R interface",
    "text": "R interface\nFigure 1 below illustrates what RStudio looks like. As well as the different menus at the top, the interface is divided in four panels. We will use the upper-left panel to write the code that instructs R to perform the analyses. You therefore need to open a new R Script (from the menu or clicking the icon). While it is also possible to directly type the commands in the console, using a script allows easily saving and replicating our work later on. While most of the results will show up in the console in the lower-left panel, the tab plots in the lower-right panel reproduces the visualisations we implement (by contrast, the tab files shows the structure of the directory where we are working on). Lastly, the environmment, located in the upper-left panel displays the objects that we load into the system.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: RStudio Interface.\n\n\n\nAs mentioned above, we will use a script to document our work. Using scripts makes it easy to keep a record of the commands you use, which in turn facilitates replicating (or adjusting) your analyse later, which will save you so much time (you can also re-use your old scripts or borrow parts to use in other projects)."
  },
  {
    "objectID": "get-r.html#setting-up-the-stage",
    "href": "get-r.html#setting-up-the-stage",
    "title": "Intro to R",
    "section": "Setting up the stage",
    "text": "Setting up the stage\nGiven that R needs some basic information regarding which folder in our computer (or the cloud) we are working from and which set of tools we are going to use, scripts usually start in a similar way. The code below includes some useful preparatory commands. You can copy and paste it in your own script.\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# Set working directory\ngetwd() # Provides the current working directory\nsetwd(\"~/Documents/quants\") \n  # sets the working directory to the appropriate folder\n\n# Install packages\ninstall.packages(\"tidyverse\")\n\n# Load packages\nlibrary(tidyverse)\n\nNotice that the symbol # allows creating “comments” (the characters turn green). R does not “read” these lines when implementing the code, so they can be used to both better structure our scripts and comment the code itself.\nA couple of further clarifications are though in order. Firstly, it is often difficult to type the correct directory path (the one above is the one in my computer). Setting it manually through the menu helps properly selecting the folder you want to be working from: Session/Set working directory/Choose directory... Implementing this operation actually runs the necessary command in the console, so you can actually see the path to your folder or copy and paste it into the script (as it will be type in the script, you won’t need to do this again). I cannot emphasise enough how important this step is, since it indicates R where in our computer we will be working (so it is easy to access the necessary files). In this regard, it is important to have a folder structure (data, results, etc.) that facilitates navigating through your files.\nSecondly, the way that R works is by relying on tools that are contained in different packages. There are many of these packages and they not only need to be installed (only once), but also open before using (each session). This is why we need the commands install.packages() and library() to indicate which packages need to be installed and opened. The package tidyverse, in particular, gathers together different packages that are commonly used like ggplot2, dplyr, readr, etc. (it is therefore not necessary to install and load those packages individually). We will keep incorporating different packages as we need them.\nYour RStudio interface should look like something like Figure 2, except for the fact that the command setwd() should include the path to your own working directory. We can now run these lines of code selecting them and using the icon Run. Clicking it makes R go through those command lines in sequence (or through the whole script if you indicate so). It first clears the environment, then sets the working directory and lastly installs and opens the package tidyverse. The results of these commands happen in the console panel, which gives some comments that we can safely ignore for now.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Starting a R script."
  },
  {
    "objectID": "get-r.html#importing-data-sets",
    "href": "get-r.html#importing-data-sets",
    "title": "Intro to R",
    "section": "Importing data sets",
    "text": "Importing data sets\nAs well as the appropriate software (R and RStudio), this session uses two historical data sets to illustrate the concepts and methods covered here (you should have downloaded them). In order to start exploring this information, we first need to import the data into the R environment.\nAs an illustration, let’s focus on the Paisley dataset, one of the historical sources we will be exploring here. The raw data is stored as an excel spreadsheet named “paisley-data.xlsx” in the folder “data”. The command readxl() imports this excel file into the R environment. Although readxl is contained in the tidyverse package, you still need to open it explicitly using library().\n\nlibrary(readxl)\ndata &lt;- read_excel(\"data/paisley-data.xlsx\")\n\nNotice that we are instructing R to find the file that is located in a particular folder. We are using a relative path that stems from where your project is saved in (if the data is in the same working directory, you don’t need the path, just the file name).1 Notice also the symbol &lt;- (called assignment operator). It serves to create a (temporary) object, named data containing the Paisley data, which is now in the “environment” we are working with. The name of the object is up to you, we call it “data” but it could be anything else with certain restrictions (i.e. not starting with a number). One neat R feature is that you can load many objects simultaneously (with different pieces of information each) and treat them separately depending on your needs (we will see that we can “create” those objects ourselves as results of our analyses).\nOnce an object has been created containing the data frame, it is listed in the upper-right window called environment. Typing the name of the object (data in this case) provides a peak at the underlying information. As shown below, the upper left corner indicates that this data frame (referred to as a tibble in the tidyverse terminology) contains 1,000 individuals (rows) and 21 fields (columns). By default, typing the name of the object (data) only provides information on the 10 first cases in order to save memory and space (imagine that your dataset contains 10 million observations!). The number of fields that are displayed depends on how much space is available in the console in the lower part of the interface. You are nonetheless informed about the number of rows and variables that are not visible (as well as the names of those variables). Notice also that, below the variable name, R also indicates the type of variable: some are categorical (“chr” meaning character) and others are numerical (“dbl” meaning double).\n\ndata\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n# ℹ 990 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nIf you want to display more cases, you can use the function print(n = 15) and indicate the number of cases to be reported. Let’s pause for a moment to disect what the code below is doing. Notice first that it is crucial to indicate where the information you are asking for is coming from. Remember that we imported the Paisley data into the object name data. The pipe (|&gt;) here basically takes this object and uses it as input in the next line of code, which uses the function print() to request listing the first 15 cases from that object. Alternatively, you can have a look at the last 20 cases by typing tail(20).\n\ndata |&gt;  \n  print(n = 15)\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n11    11   343 may       1841 AGNES    CURRIE… fema…    35 islay scotland paisl…\n12    12   382 june      1841 ELLIZA   MUNN    fema…    18 john… scotland johns…\n13    13   425 june      1841 SARAH    BLACK … fema…    36 glas… scotland kelvi…\n14    57     3 january   1841 THOMAS   ROBERT… male     19 pais… scotland high …\n15    58    19 january   1841 JOHN     MONTGO… male     24 some… england  barra…\n# ℹ 985 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nThere are other commands that help knowing more about how the data set looks like such as glimpse() or names(). Feel free to try them out yourself. You can also have a sense of the magnitude and complexity of the whole data set by typing view(data). The latter opens up a new tab where you can explore the full data set at ease. Notice also that some values are missing. R signals them as NA (not available), meaning that no information is recorded in those fields for those observations. We will discuss the importance of missing values in due time. What it is important to stress now is that, although the Paisley data set is not especially big, scrolling up-down and left-right makes it obvious that it is extremely difficult to extract any kind of pattern by just “looking” at all this information. Here is where statistics (and R) come to the rescue. This is basically what this session will be about: a basic overview on how historians use computational methods to extract the rich information contained in our sources. If you have got this far, congrats, you are ready for it!"
  },
  {
    "objectID": "get-r.html#further-references",
    "href": "get-r.html#further-references",
    "title": "Intro to R",
    "section": "Further references",
    "text": "Further references\nAlexander, Rohan (2023), Telling stories with Data. With applications in R (CRC Press).\nIsmay, Chester and Kim, Albert Y. (2024), Statistical inference via Data Science. A ModernDive into R and the Tidyverse (CRC Press).\nSilge, Julia, and Robinson, David (2017), Text mining with R. A tidy approach (O’Reilly).\nWickham, Hadley, Çetinkaya-Rundel, Mine, and Grolemund, Garret (2023), R for Data Science (O’Reilly; 2nd edition)."
  },
  {
    "objectID": "get-r.html#footnotes",
    "href": "get-r.html#footnotes",
    "title": "Intro to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will work assuming the working directory is set to the appropriate folder (see previous step above). Although we strongly advice not to use menus, finding the appropriate path to your files is not always straightforward, so we make an exception here.↩︎"
  },
  {
    "objectID": "other-data.html",
    "href": "other-data.html",
    "title": "Sources of spatial data",
    "section": "",
    "text": "Below you can find a curated set of varied datasets that you can use for the class assignments and your own research projects. Please make sure to read the underlying documentation describing the information. The list is obviously highly selected based on my own interests and availability, so you are also free to choose your own sources.\nDifferent R packages also make it easy to import spatial objects. For instance, the packages geodata, spData, rnaturalearth or maps facilitate access to climate, crops, elevation, land use, soil, administrative boundaries and other data ((moraga2023?) surveys some of these packages here). National agencies will also have shapefiles with all settlements and many other spatial features. GeoNorge, for instance, provides thousands of shapefiles and raster files for Norway."
  },
  {
    "objectID": "other-data.html#shapefiles",
    "href": "other-data.html#shapefiles",
    "title": "Sources of spatial data",
    "section": "Shapefiles",
    "text": "Shapefiles\n\nGADM provides boundaries at different administrative levels for all countries.\nNatural Earth."
  },
  {
    "objectID": "other-data.html#raster-data",
    "href": "other-data.html#raster-data",
    "title": "Sources of spatial data",
    "section": "Raster data:",
    "text": "Raster data:\n\nWorldClim: historical climate data (1970-2000).\nFAO-GAEZ: suitable agricultural land and crop suitability indexes.\nSRTM Digital Elevation Database\nEuropean Soil Database\nNASA Earth Data: night-light luminosity.\n\nThese examples provide global or international datasets. National agencies may have their own datasets.\nThe packages geodata or rnaturalearth also facilitate importing this kind of information into R."
  },
  {
    "objectID": "other-data.html#textual-corpuses",
    "href": "other-data.html#textual-corpuses",
    "title": "Sources of spatial data",
    "section": "Textual corpuses",
    "text": "Textual corpuses\nAs discussed during the course, there are computational tools that allow extracting locations from textual corpuses (entity recognition) and then assign them geographic coordinates (geo-coding). The package tidygeocoder “makes getting data from geocoding services easy” (cambon2021?). As illustrations, this kind of tools have been employed in the following projects:\n\nThe Trading Consequences project (clifford2016?): extracting a vast amount of information on the geographical location of commodities exchanged in the British economic world during the long 19th century (1789-1914).\nIn the Mapping the State of the Union, Mitch Fraas and Ben Schmidt extract the locations mentioned in the 224 State of Union addresses delivered yearly since 1790.\nThe emotions of London project text-mines place-names from 18th- and 19th-century novels and the emotions they elicit in the corpus."
  },
  {
    "objectID": "other-data.html#aerial-imagery-satellite-images",
    "href": "other-data.html#aerial-imagery-satellite-images",
    "title": "Sources of spatial data",
    "section": "Aerial imagery / Satellite images",
    "text": "Aerial imagery / Satellite images\nNational agencies started to survey their entire territory using aerial photographs in the 1930s, a practice that continued to the present and therefore constitute an important source of historical information (with millions of aerial photos at multiple points in time). Recent examples using historical aerial photographs are (sylvester2012?), (midgley2017?), (pinol2018?), (carvalho2021?) and (llena2023?) that track the temporal evolution of urban areas, crop fields, glacial dynamics, coastal erosion and forest cover (and land abandonment).\nMore recently, satellite imagery have become publicly available and have open up completely new ways of doing research. As well as high spatial resolution and global geographic coverage, these remote sensing technologies provide information that it is difficult to obtain by other means (donaldson2016?). The LANDSAT program was launched in 1972 and other programmes joined in in the 1990s and later, so contemporary historians can make use of these technologies to provide visual evidence, as well as comparing images taken at different periods and track changes in land cover and quality, night lights, topography, deforestation, pollution, drought, weather and climatic fluctuations, etc. Within the social sciences, this information has been primarily employed by economists; see (donaldson2016?) and (wuepper2025?) for surveys of recent literature. Likewise, (munteanu2024?) stresses the potential of globally available black-and-white satellite photographs available from the 1960s."
  },
  {
    "objectID": "other-data.html#scanned-historical-maps",
    "href": "other-data.html#scanned-historical-maps",
    "title": "Sources of spatial data",
    "section": "Scanned historical maps",
    "text": "Scanned historical maps\nHistorical maps contain spatial information about political and cultural borders, transport infrastructure, topographical information, land cover, buildings, etc., so they constitute a fantastic historical source. Here are some online collections:\n\nThe US Historical Topographic Map Collection\nThe David Rumsey Map Collection\nOldMapsOline\n\nAs examples of projects geo-referencing old maps, see the Viabundus Project (holterman2023?). Based on the atlas Hansische Handelsstraßen, this project has produced shapefiles containing the roads and waterways connecting northern Europe between 1350 and 1650, as well as the institutional nodes behind these transportation networks (towns and settlements, tolls, fairs, staple markets, etc.). Likewise, while (heblich2021?) relies on topographical maps published between 1880 and 1900 to extract the location of 5,000 industrial chimneys and trace atmospheric pollution patterns in British cities, (redding2024?) maps the destruction of London during the Second World War. Similarly, (siodla2015?) and (hornbeck2017?) use historical maps to understand the effects of the great fires in Boston and San Francisco. Likewise, Charles Butcher and his team (here at NTNU) rely on maps to identify the political influence of pre-colonial African states. More examples can be found in these blog posts by Alexandra Cirone and James Feigenbaum.\nDigitising old maps involves two steps: (1) geo-referencing a historical map, that is, adding real-world spatial coordinates, and (2) digitising the spatial features you are interested in using dots, lines or polygons (creating a shapefile). Although this process can be done using R, it is more intuitive using specific GIS software such as QGIS or ArcGIS. The Programming Historian and the Geospatial Historian offer great tutorials both in QGIS and ArcGIS (clifford2013?; see also gregory2007?). Manually digitising points, lines or polygons can nonetheless be a time-consuming activity. Alternatively, advances in computational methods enable automatically extracting digital versions from scanned images of historical maps (or aerial images). Although the combination of text and symbols (lines, polygons, etc.) still pose significant challenges to automated pattern recognition methods, this is already a very promising area (hosseini2021?; combes2022?; litvine2024?; mcdonough2024?)."
  },
  {
    "objectID": "other-data.html#historical-shapefiles-rasters",
    "href": "other-data.html#historical-shapefiles-rasters",
    "title": "Sources of spatial data",
    "section": "Historical shapefiles / rasters",
    "text": "Historical shapefiles / rasters\nHistorians have been busy creating historical GIS, so there are plenty of shapefiles already available to the public.\n\nThe China Historical GIS with placenames and administrative units for the Chinese Dynasties.\nThe Great Britain Historical GIS supplying administrative boundaries since the early 19th century.\nThe US National Historical Geographic Information System containing all levels of U.S. census geography, including states, counties, tracts, and blocks, from 1790 through the present).\nThe French Historical GIS, 1700-2020 (including administrative units, transportation networks, etc.; (litvine2023?)). See also the Mapping the Third Republic. A Geographic Information System of France (1870–1940) (gay2020?).\nHistorical regional boundaries and transportation infraestructure in Europe since the mid-19th century (marti2023?).\n\nLikewise, different websites have collected lists of national historical GIS, as well as examples of projects using GIS tools, such as The Historical GIS Research Network or Geospatial Historian.\n\nHistorical gazetteers. The project A vision of Britain through time has gathered around 2 million historical place names from the early 19th century onwards. The Digital Gazetteer of the Song Dynasty (906-1276 CE) (mostern2022?). Pleiades, a community-built gazetteer of ancient places. Similarly, the project ESPAREL has extracted and geo-referenced the almost 20,000 population entities existing in the 1887 Spanish nomenclator and link them with their current and past counterparts (esparel2022?). The World Historical Gazetteer is a platform that hosts many of these initiatives geo-locating historical place names across the world.\nHistorical Climatology\nShips’ logbooks are a especially valuable source since their entries not only recorded the vessels’ geographical position (longitude and latitude), but also systematic meteorological information (and other events, such as whales seen or captured, etc.) daily or even several times a day (smith2012?; garcia2018?; walker2024?). See also the Whaling History, the Weather Time Machine or the Old Weather projects.\nThe Historical Settlement Data Compilation for the United States (HISDAC-US) (uhl2021?; connor2020?). Historical gridded settlement layers derived from property records since 1810. These files count the number of built-up properties devoted to different uses (agricultural, commercial, industrial, residential, etc.) per grid cell and therefore allows tracking the evolution of urbanization and land Use use over time.\n\nThese are just some examples. A fine-grain online search, specifying the area, the period and the topic of interest may also produce the desired results. Moreover, plenty of research, either by public institutions or individual researchers, has also used GIS tools but has not made the underlying data public. Most maps published nowadays in books, academic journals, newspapers and websites make use of these tools and therefore are based on shape or raster files that can be shared and reproduced. Authors are usually happy to share their materials providing they are properly referenced, so contacting them is always advisable."
  },
  {
    "objectID": "other-data.html#historical-datasets-including-spatial-coordinates",
    "href": "other-data.html#historical-datasets-including-spatial-coordinates",
    "title": "Sources of spatial data",
    "section": "Historical datasets including spatial coordinates",
    "text": "Historical datasets including spatial coordinates\nAs well as historical locations themselves, there are also plenty of examples of historical information that has also been geo-located.\n\nVOC Dataset (Petram et al. 2024): This dataset stores the pay ledgers of the Dutch East India Company’s (VOC), primarily from the eighteenth century. It contains almost 800,000 records containing each crew member’s name, place of origin, rank, wage, etc. The raw information has been carefully curated and stored in several .csv files that can be merged together using the corresponding IDs. Read more about this source here.\nTudor Network of Power (Ahnert et al. 2023). This data contains all (surviving) items of correspondence in the Tudor State Papers (1509-1603), which are the official government records of the Tudor period in England. As explained by the authors (Ahnert and Ahnert 2023), data cleaning and curation constituted a significant effort. As well as more traditional quantitative methods, this data set is suited for the network analysis.\nTheater History of Operations Reports provides 4,8 million observations defined by the position of an aircraft bombing a particular target in the Vietnam War between 1965 and 1975.\nA brief history of human time (Laouenan et al. 2022). This database includes information on 2.2 million notable individuals born between 3500BC and 2020 (5,500 years of human history) collected from Wikipedia and other secondary sources. As well as dates of birth and death, the data set includes place of birth and other features characterising these individuals (when available). As the authors document, Anglo-Saxon personalities are over-represented due to the bias naturally present in existing projects based on the English edition of Wikipedia. See also Schich et al. (2014) who used the dates of birth and death of a subsample of this data (150,000 notable individuals) to map the evolution of European cultural history during the last 2,000 years.\nAcademich scholars and literati in Medieval and Early Modern Europe (De La Croix, n.d.). Relational database on around 83,000 scholars and literati active in European Academia between 1000 and 1800. As well as place and year of birth and details, it details to which institutions these individuals belonged (universities, scientific academies, etc.). See De La Croix, Scebba, and Zanardello (2025) and De La Croix and Morault (2025) for two applications using social network analysis.\n\nAgain, a targeted online search may yield results specific to your interests. Although searching for area and period of study is always useful, many topics are also very well covered: population, education (cappelli2023?), social conflicts (Chambru and Maneuvrier-Hervieu 2022), lighthouses (bogart2022?), sailing routes and wrecks (here), to mention only a few.\nAnother alternative is to rely on contemporary shapefiles. Physical features (i.e. rivers, coastlines, etc.) are not likely to have changed much, so you easily find appropriate GIS files online or any national agency. Likewise, many historical locations (i.e. settlements, regional entities, etc.) still exist and are contained in contemporary geo-referenced databases.\nAlternatively, spatial coordinates can be gathered from GPS receivers, online searches or google map itself. Opening Google Maps and clicking in any point provides this information. Notice though that google maps reports latitude first and longitude second, so the order is switched. This kind of information is, for instance, very important for recording archaeological locations."
  },
  {
    "objectID": "other-data.html#norwegian-data",
    "href": "other-data.html#norwegian-data",
    "title": "Sources of spatial data",
    "section": "Norwegian data",
    "text": "Norwegian data\nThe Kommunedatabasen also has digitised a huge amount of historical information on municipalities (kommuner). You can request shapefiles with the (changing) municipal boundaries from 1880 onwards.\nOther additional sources can be found below:"
  },
  {
    "objectID": "other-data.html#miscellaneous",
    "href": "other-data.html#miscellaneous",
    "title": "Sources of spatial data",
    "section": "Miscellaneous",
    "text": "Miscellaneous\nThose students with other research interests can choose their dataset on their own. The possibilities are endless. Here are just a few examples:\nAs mentioned above, I encourage you to find your own dataset."
  },
  {
    "objectID": "r-scripts/chall_reg.html",
    "href": "r-scripts/chall_reg.html",
    "title": "4.2 Challenges to regression",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here."
  },
  {
    "objectID": "r-scripts/mult_reg.html",
    "href": "r-scripts/mult_reg.html",
    "title": "4.1. Multiple regression analysis",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"xlsx\")\n# install.packages(\"writexl\")\n\n# Load the packages you need\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(modelr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(modelsummary)\nlibrary(marginaleffects)\n\n# Import the data\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\n\n\n#### MULTIPLE REGRESSION ANALYSIS \n\n# Regression analysis goes beyond and allows:\n# (1) assessing the actual impact of X on Y: coefficient b\n# (2) computing how much of the variation in Y is explained by X (or Xs): R-squared\n# (3) allows controlling directly for the effect of other variables ***********************\n\n# Prepare the data (just in case)\ndata &lt;- data %&gt;%\n  mutate(\n    weight_kg = 0.453592*weight,\n    height = 30.48*feet+2.54*inches,\n    male = if_else(sex==\"male\", 1, 0),\n    england = if_else(countryb==\"england\", 1, 0),\n    ireland = if_else(countryb==\"ireland\", 1, 0),\n    scotland = if_else(countryb==\"scotland\", 1, 0),\n    overseas = if_else(countryb==\"overseas\", 1, 0),\n    read = recode(lit_adj,\n                  \"illiterate\" = 0,\n                  \"read\" = 1,\n                  \"write\" = 1),\n    write = if_else(lit_adj==\"write\", 1, 0),\n    urban = case_when(born==\"glasgow\" ~ 1,\n                           born==\"port glasgow\" ~ 1,\n                           born==\"edinburgh\" ~ 1, \n                           born==\"liverpool\" ~ 1,\n                           born==\"dublin\" ~ 1,\n                           born==\"london\" ~ 1,\n                           born==\"barcelona\" ~ 1,\n                           born==\"birmingham\" ~ 1,\n                           born==\"manchester\" ~ 1,\n                           TRUE ~ 0))\n\n# Multiple explanatory variables\nreg &lt;- data %&gt;%\n  filter(age&gt;=20 & countryb!=\"overseas\") %&gt;% \n  lm(weight_kg ~ age + height + male + write, data = .)\n\nget_regression_summaries(reg) %&gt;%\n  select(nobs, r_squared, adj_r_squared) # R-squared\n\nreg %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate) # regression coefficients\n\nreg %&gt;%\n  get_regression_table() %&gt;%\n  select(!statistic) # tables including standard errors, confidence intervals, etc.\n\n\n# Visualising regression results\n\ndata %&gt;%\n  filter(age&gt;=20 & countryb!=\"overseas\") %&gt;% \n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n  \n\n## two possibilities\nlibrary(marginaleffects)\nreg %&gt;%\n  plot_predictions(\"age\") # the link between one X and Y \n\nlibrary(modelsummary)\n\nmodelplot(reg, coef_omit = 'Interc') \n  # all the coefficients together\n  # all the coefficients simultaneously\n\n### What happens to the model when we add more explanatory variables??\n  # (1) it usually increases the explanatory power: R-squared \n  # (2) the effect of the other variables may change \n  # (3) the estimations may become noisier \n\n# Illustration: three regresssions explaining heights (restricted sample)\nsample &lt;- data %&gt;%\n  filter(age&gt;=20 & age&lt;=30 & countryb!=\"overseas\" & countryb!=\"ireland\")\n\nreg1 &lt;- sample %&gt;%\n  lm(height ~ write, data = .)\nreg2 &lt;- sample %&gt;%\n  lm(height ~ write + male, data = .)\nreg3 &lt;- sample %&gt;%\n  lm(height ~ write + male + england, data = .)\n\nmodelsummary(\n  list(\n    \"M1\" = reg1,\n    \"M2\" = reg2,\n    \"M3\" = reg3),\n  statistic = c(\"s.e. = {std.error}\", \"p-value = {p.value}\"),\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"),\n  fmt = 3,\n  coef_omit = \"Intercept\",\n  note = \"For simplicity, the intercept is omitted.\") \n\n## (1) Adding more variables increase the R-squared providing the new variables are relevant. \n\n## (2) Adding an additional variables to the model may change the coefficients of the other variables. \n  # omitted variable bias: if an omitted variable (Z) is correlated with both X and Y, \n    # the regression coefficient is unreliable:\n      # b not only reflects the effect of X on Y, but it also partly captures the effect of Z on Y\n    # controlling for Z (including Z in the model) addresses this issue\n\n# the effect of adding additional variables to the existing coefficients basically reflects:\n  # --the correlation between the added variable and the dependent variable (Y)\n  # --the correlation between the added variable and the previously existing explanatory variables.\n\n# correlation matrix\ndata %&gt;%\n  filter(age&gt;=20 & age&lt;=30 & countryb!=\"overseas\" & countryb!=\"ireland\") %&gt;%  \n  select(height, write, male, england) %&gt;%\n  datasummary_correlation()\n\n## (3) The estimations may become noisier\n\n# adding more variables increases the number of coefficients to be estimated with the same information (sample size). \n  # reducing the degrees of freedom\n\n# the regression coefficients are computed using only independent information (exclusive to one particular variable). \n  # If two of the explanatory variables are correlated between each other, they share information. \n  # The procedure behind regression cannot use the shared information to estimate their coefficients, \n  # so that information is discarded and $b$ is estimated using only the *independent* information remaining. \n  # Having less information increases the standard errors, thus reducing the accuracy of the estimations: \n  # confidence intervals wider and p-values become larger\n\n\n## Regression tables: very useful to compare several models in just one table  \n  # assess how our results behave under different specifications: \n    # including more/less variables\n    # differences across groups or time-periods, \n    # excluding outliers, \n    # etc. \n\nreg_all &lt;- data %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age + sex + height + england + ireland + overseas, data = .)\nreg_males &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"male\") %&gt;%\n  lm(weight_kg ~  age + height + england + ireland + overseas, data = .)\nreg_females &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"female\") %&gt;%\n  lm(weight_kg ~ age + height + england + ireland + overseas, data = .)\nreg_all2 &lt;- data %&gt;%\n  filter(age&gt;=20 & age&lt;=50 & countryb!=\"overseas\") %&gt;%\n  lm(weight_kg ~ age + sex + height + england + ireland + overseas, data = .)\nreg_males2 &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"male\" & age&lt;=50 & countryb!=\"overseas\") %&gt;%\n  lm(weight_kg ~  age + height + england + ireland + overseas, data = .)\nreg_females2 &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"female\" & age&lt;=50 & countryb!=\"overseas\") %&gt;%\n  lm(weight_kg ~ age + height + england + ireland + overseas, data = .)\n\nmodelsummary(\n  list(\n    \"1. All\" = reg_all,\n    \"2. Males\" = reg_males,\n    \"3. Females\" = reg_females,\n    \"4. All\" = reg_all2,\n    \"5. Males\" = reg_males2,\n    \"6. Females\" = reg_females2),\n  statistic = \"std.error\",\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\"),\n  coef_omit = \"Intercept\",\n  fmt = 2,\n  note = \"For simplicity, the intercept is omitted.\")\n\n\n\n## Quantifying time (controlling for other confounders)\n\nsample &lt;- data %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  filter(age&gt;=20 & year_birth&gt;1795)\n\nm1 &lt;- sample %&gt;%\n  lm(height ~ year_birth, data = .)\n\nm2 &lt;- sample %&gt;%\n  lm(height ~ year_birth + sex + england + ireland + overseas, data = .)\n\nmodelsummary(\n  list(\n    \"(1)\" = m1,\n    \"(2)\" = m2),\n  statistic = \"std.error\",\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\"),\n  coef_omit = \"Intercept\",\n  fmt = 2,\n  note = \"For simplicity, the intercept is omitted.\")  \n\n\n\n### Choosing which variable introduce in the analysis should be theoretically justified\n\n# Also:\n  # introducing additional *relevant* variables (Z) is helpful for two main reasons:\n    # -- adds more information, thus improving the accuracy of our estimations \n          # (if X and Z are not correlated)\n    # -- it prevents the **omitted variable bias** if X and Z are correlated. \n          # if an omitted variable (Z) is correlated with both X and Y, \n            # the regression coefficient is unreliable\n  \n  # however, adding more variables may also reduce accuracy due to:\n    # -- reducing the degrees of freedom (df=n-k-1)\n    # -- multicolinearity problems: \n      # if X and Z are correlated, coefficients are estimated with less *independent* information, \n        # which increases the standard errors and reduces the likelihood of being statistically significant \n          # (confidence intervals also become wider)\n\n  # The omitted variable bias is arguably a much worse problem than multicolinearity, \n\n\n\n### Challenges to regression analysis\n  # Non-linear relationships (functional form)\n  # Parameter stability\n  # Outliers\n\n### Correlation is not causation\n# Role of omitted variables\n# Reverse causality\n\n### Other issues\n# Number of observations (noise)\n# Categories employed\n# Garbage in, garbage out (the results are as good as the data itself)\n\n\n# Functional form:\ndata %&gt;%\n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nreg &lt;- data %&gt;%\n  mutate(age_sq = age*age) %&gt;%\n  lm(weight_kg ~ age + age_sq, data = .)\n  \ndata %&gt;%\n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2))\n\ndata %&gt;%\n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth()"
  },
  {
    "objectID": "r-scripts/desc_bivar.html",
    "href": "r-scripts/desc_bivar.html",
    "title": "1.3. Comparing dimensions (bivariate statistics)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n##### Bi-variate statistics #######\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# upload basic packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\n# import data\ndata &lt;- read_excel(\"data/paisley_data.xlsx\")\n\ndata2 &lt;- read_rds(\"paisley_v2.rds\")\n\n\n### Qualitative variables\n\n# Cross-tabulation / contingency table\nview(data)\n\ndata %&gt;%\n  count(employed)\n\ndata %&gt;%  \n  count(sex, employed)\n\ndata %&gt;%  \n  count(sex, employed) %&gt;%\n  pivot_wider(names_from = employed, values_from = n)\n\n\ntable3 &lt;- data %&gt;%  \n  count(sex, employed) %&gt;%\n  pivot_wider(names_from = employed, values_from = n) %&gt;%\n  mutate(total = employed + unemployed,\n         emp_perc = 100*employed/total,\n         unemp_perc = 100*unemployed/total) %&gt;%\n  mutate_if(is.numeric, round, 1) \n\ntable3 &lt;- table3 %&gt;%\n  relocate(sex, employed, emp_perc, unemployed, unemp_perc, total)\n\ntable3\n\n\n# Visualisation\n\ndata %&gt;% \n  filter(countryb == \"scotland\" & age&gt;=16) %&gt;%  \n  ggplot(aes(x = literacy)) + \n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~ sex, nrow = 1)\n\ndata %&gt;% \n  filter(countryb == \"scotland\" & age&gt;=16) %&gt;%  \n  ggplot(mapping = aes(x = literacy, y = after_stat(prop), group = 1)) + \n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~ sex, nrow = 1)\n\ndata %&gt;% \n  filter(countryb == \"scotland\" & age&gt;=16) %&gt;%\n  ggplot(aes(x = literacy, fill = sex)) + \n  geom_bar(position = \"dodge\") +\n  coord_flip()\n\n?ggplot\n\n# 3 variables simultaneously\n\ndata &lt;- data %&gt;% \n  mutate(lit_rank = factor(literacy, \n                           levels = c(\"illiterate\", \n                                      \"read a little\", \n                                      \"read tolerably\", \n                                      \"read well\", \"cannot write\", \n                                      \"read & write a little\", \n                                      \"read & write tolerably\", \n                                      \"read & write well\", \n                                      \"superior education\"), \n                           ordered = TRUE))\n\ndata %&gt;% \n  filter(countryb == \"scotland\" | countryb == \"ireland\") %&gt;%\n  ggplot(aes(x = lit_rank, fill = sex)) + \n    geom_bar(position = \"dodge2\") +\n    coord_flip() +\n    facet_wrap(~ countryb, nrow = 1)\n\n\n### Comparing numerical variables by group\n\n# Means\ndata\n\ndata &lt;- data %&gt;%\n  mutate(feet = replace(feet, feet == 50, 5))\n\ndata &lt;- data %&gt;%\n  mutate(height = 30.48*feet + 2.54*inches)\n\ndata %&gt;%\n  ggplot(aes(x = height)) + \n  geom_histogram()\n\ndata %&gt;% filter(feet&gt;10) %&gt;% select(feet, inches)\n\n\n\nView(data)\n\ndata %&gt;% \n  filter(countryb==\"ireland\" | countryb==\"scotland\" | countryb==\"england\") %&gt;%\n  group_by(countryb) %&gt;%\n  summarize(\n    obs = sum(!is.na(height)),\n    mean_height = mean(height, na.rm = TRUE)) \n\n# %&gt;% mutate_if(is.numeric, round, 3)\n\n# Distributions\n\ndata %&gt;%   \n  filter(countryb==\"ireland\" | countryb==\"scotland\" | countryb==\"england\") %&gt;%\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~ countryb, nrow = 1)\n# also very different sample sizes\n\ndata %&gt;%   \n  filter(countryb==\"ireland\" | countryb==\"scotland\" | countryb==\"england\") %&gt;%\n  ggplot(mapping = aes(x = height, y = ..density..)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~ countryb, nrow = 1)\n# difficult to eyeball differences between graphs\n\n# Overlapping histograms (kernel density graphs)\ndata %&gt;%   \n  ggplot(aes(x = height, y = ..density..)) +\n  geom_histogram(binwidth = 5) +\n  geom_density(kernel = \"gaussian\", color = \"red\", size = 1.5)\n\ndata %&gt;%   \n  filter(countryb==\"ireland\" | countryb==\"scotland\" | countryb==\"england\") %&gt;%  \n  ggplot(aes(x = height, colour = countryb)) +\n  geom_density()\n\n# Boxplots\n# another way of comparing distributions\n\ndata %&gt;%   \n  filter(countryb==\"ireland\" | countryb==\"scotland\" | countryb==\"england\") %&gt;%  \n  ggplot(aes(x = countryb, y = height)) +\n  geom_boxplot()\n\n# the solid line depicts the median value (50th percentile)\n# the box contains 50 per cent of the observations \n# (those contained between the percentiles 25th and 75th (IQR).\n# the vertical lines that extend below and above the box \n# reflect observations falling within 1.5 the interquartile range\n# the black dots refer to the extreme values that all outside that range \n\ndata %&gt;%   \n  filter(countryb==\"ireland\" | countryb==\"scotland\" | countryb==\"england\") %&gt;%  \n  ggplot(aes(x = countryb, y = height)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2, width=0.10, height=0.05)\n\n\n## Compositional effects\n\ndata %&gt;% \n  filter(countryb==\"ireland\" | countryb==\"scotland\" | countryb==\"england\") %&gt;%\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) %&gt;%\n  group_by(countryb) %&gt;%\n  summarize(\n    obs = sum(!is.na(height)),\n    mean_height = mean(height, na.rm = TRUE),\n  )\n\n\n## With qualitative variables\n\nlit_sex_countryb &lt;- data %&gt;% \n  mutate(write = ifelse(lit_adj==\"write\", 1, 0)) %&gt;%\n  filter(countryb %in% c(\"england\", \"ireland\", \"scotland\")) %&gt;%\n  group_by(sex, countryb) %&gt;%\n  summarize(\n    obs = sum(!is.na(write)), \n    write = mean(write, na.rm = TRUE)) %&gt;% \n  mutate_if(is.numeric, round, 3) %&gt;%\n  pivot_wider(names_from = sex, values_from = c(\"obs\", \"write\")) %&gt;%\n  relocate(countryb, obs_male, write_male, obs_female, write_female)\nlit_sex_countryb\n\n\n## Comparing multiple groups\n\ndata %&gt;% \n  group_by(sex, countryb) %&gt;%\n  summarize(\n    obs = sum(!is.na(height)), \n    height = mean(height, na.rm = TRUE)) %&gt;% \n  mutate_if(is.numeric, round, 2) %&gt;%\n  pivot_wider(names_from = sex, values_from = c(\"obs\", \"height\")) %&gt;%\n  relocate(countryb, obs_male, height_male, obs_female, height_female)\n\n\n# Visually\n\ndata %&gt;% \n  group_by(sex, countryb) %&gt;%\n  summarize(\n    obs = sum(!is.na(height)), \n    height = mean(height, na.rm = TRUE)) %&gt;% \n  ggplot(aes(x = countryb, y = height, color = sex)) +\n  geom_point()\n\n### Numerical variables\n\n# Tables (class intervals)\n\ndata &lt;- data %&gt;%\n  mutate(weight_kg = 0.453592*weight)\n\ndata %&gt;% \n  filter(age&gt;=20) %&gt;%\n  mutate(age_class = cut(age, breaks = seq(20, 90, 10))) %&gt;%\n  group_by(age_class) %&gt;%\n  summarise(obs = sum(!is.na(weight_kg)),\n            mean_weight = mean(weight_kg, na.rm = TRUE))\n\n# Line graph\ndata %&gt;% \n  filter(age&gt;=20) %&gt;%\n  mutate(age_class = cut(age, breaks = seq(20, 90, 10))) %&gt;%\n  group_by(age_class) %&gt;%\n  summarise(obs = sum(!is.na(weight)),\n            mean_weight = mean(weight, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = age_class, y = mean_weight, group = 1)) +\n  geom_line()\n\n# Scatter plot\ndata %&gt;%\n  filter(age&gt;=20) %&gt;%\n  ggplot(aes(x = age, y = weight)) +\n  geom_point()\n\ndata %&gt;%\n  filter(age&gt;=20) %&gt;%\n  ggplot(aes(x = age, y = weight_kg)) +\n  geom_point() +\n  geom_smooth(SE = FALSE) +\n  stat_summary(geom = \"line\", fun.y = \"mean\", color = \"red\")   \n\n\n# distinguishing by a third dimension\ndata %&gt;%\n  filter(age&gt;=20) %&gt;%\n  ggplot(aes(x = age, y = weight_kg, color = sex)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n### Evolution over time (history!)\n\n# a table\nheight_year &lt;- data %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(year_birth) %&gt;%\n  summarise(obs = sum(!is.na(height)),   \n            av_height = mean(height, na.rm = TRUE))\nheight_year\n\n# visually\nheight_year %&gt;%\n  ggplot(aes(x = year_birth, y = av_height)) +\n  geom_point(col = \"red\") +\n  geom_line()\n\n# creating bins\ndata %&gt;% \n  mutate(year_birth = year - age) %&gt;%\n  mutate(year_birth_5 = cut(year_birth, breaks =  seq(1795, 1885, 5))) %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(year_birth_5) %&gt;%\n  summarise(\n    av_height = mean(height, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = year_birth_5, y = av_height)) +\n  geom_point() + geom_line(group = 1)  +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n# Given that the variable capturing the 5-year cohorts is a factor, \n# we need `group = 1` to let `ggplot` know that all the observations \n# should be treated as part of the same group, so the line can connect the points.\n\n# Smoothing the trend\nheight_year %&gt;%\n  ggplot(aes(x = year_birth, y = av_height)) +\n    geom_smooth(se = FALSE) +\n    geom_point() +\n    geom_line()\n\n# Comparing groups\ndata %&gt;% \n  mutate(year_birth = year - age) %&gt;%\n  filter(countryb==\"scotland\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(year_birth, sex) %&gt;%\n  summarise(\n    av_height = mean(height, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = year_birth, y = av_height, color = sex)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n# Other statistics\n\ndata %&gt;%  \n  mutate(year_birth = year - age) %&gt;%\n  filter(age &gt;= 18 & sex==\"male\" & year_birth&gt;=1800) %&gt;%\n  group_by(year_birth) %&gt;%\n  summarise(sd = sd(height, na.rm = TRUE),\n            mean = mean(height, na.rm = TRUE)) %&gt;% \n  mutate(cv = sd/mean) %&gt;%\n  ggplot(aes(x = year_birth, y = cv)) +\n  geom_point() +\n  geom_line() + \n  geom_smooth(se = FALSE)\n\n# qualitative variables: dummy variables\n\ndata %&gt;%\n  mutate(write = ifelse(lit_adj==\"write\", 1, 0)) %&gt;%\n  mutate(year_birth = year - age) %&gt;%  \n  filter(age&gt;=18 & age&lt;=50) %&gt;%\n  group_by(year_birth, sex) %&gt;%\n  summarise(\n    mean = mean(write, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = year_birth, y = mean, color = sex)) +\n  geom_smooth(se = FALSE) +\n  geom_point()"
  },
  {
    "objectID": "r-scripts/text_advanced.html",
    "href": "r-scripts/text_advanced.html",
    "title": "5. Advanced automated textual analysis",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages\n# install.packages(\"tidyverse\")\n# install.packages(\"tidytext\")\n# install.packages(\"tokenizers\")\n# install.packages(\"stm\")\n# install.packages(\"SnowballC\")\n\nif (!requireNamespace(\"xfun\")) install.packages(\"xfun\")\nxfun::pkg_attach2(\"tidyverse\", \"lubridate\", \"rvest\", \"stringr\", \"readtext\", \"tesseract\", \"tidytext\", \"SnowballC\", \"wordcloud\", \"wordcloud2\", \"widyr\", \"quanteda\", \"quanteda.textstats\", \"magrittr\", \"pdftools\", \"devtools\", \"tsne\", \"topicmodels\", \"readtext\")\n# The following two packages have Java dependencies that might give some (especially Windows) machines trouble. No worries if they don't load on your computer, I'll demonstrate running them on RStudio Cloud.\nxfun::pkg_attach2(\"tabulizer\", \"openNLP\")\ninstall.packages(\"devtools\")\ndevtools::install_github(\"bmschmidt/wordVectors\")\n\n# Open packages that we will be using\nlibrary(tidyverse)\nlibrary(tidytext)\n\nlibrary(SnowballC)\n\n# Importing the data\ndata &lt;- read_csv(\"data/state_of_the_union_texts.csv\")\ndata\n\n#### TF-IDF Term frequency - Inverse document frequency\n  # Identifying words that are unique (or show up less often) to certain documents (speeches, periods, etc.)\n  # No need to \"stopwords\" because words that are common in all docs are unimportant here\n\ndata &lt;- data %&gt;%\n  mutate(period = case_when(\n  Year&lt;1914 ~ \"Pre-1914\",\n  Year&gt;=1914 & Year&lt;=1945 ~ \"World Wars\",\n  Year&gt;1945 ~ \"Post-1945\")) %&gt;%\n  mutate(period = factor(period, \n                         levels = c(\"Pre-1914\", \"World Wars\", \"Post-1945\"),\n                         ordered = TRUE))\n\ndata_token &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text) %&gt;%       # tokenize first\n  count(words, period, sort = TRUE) # count them\n\ntf_idf &lt;- data_token %&gt;%\n  bind_tf_idf(words, period, n) # distinguish documents by year\n  # the commonest words are weighted away (their tf-idf score is 0)\n\ntf_idf\n  # tf - term frequency\n  # idf - inverse document frequency\n  # tf_idf - identifies the most unique words in each year\n\n# Check the highest scores\ntf_idf %&gt;% arrange(desc(tf_idf)) \n\n# You could now identify which are the most unique words in each speech\n\n\n#### POS (Part of Speech)- Identify types of words: nouns, verbs, adjectives...\n\n# There is an in-built list of words identifying POS\nparts_of_speech\n\ndata_token &lt;- data %&gt;%\n  unnest_tokens(output = word, input = Text) %&gt;%  # the data we will be using (tokenised)\n  inner_join(parts_of_speech, relationship = \"many-to-many\") %&gt;%                 # join the two datasets using \"word\" as the \"matching\" field\n  count(pos)\ndata_token\n\n\n# There are more sophisticated POS tagging would require the context of the sentence structure\n# It is though beyond what we cover here and involve other packages (\"NLP\", \"openNLP\", \"tm\")\n# See here:\n# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n\n\n#### Sentiment analysis\n# install.packages(\"textdata\")\n\nlibrary(tidytext)\nlibrary(textdata)\n\n# Tidytext includes three dictionaries with \"sentiments\": afinn, bing & nrc\n\nget_sentiments(\"afinn\")\n  # the first time you will need to say yes to download of the sentiment dictionary\ntail(get_sentiments(\"afinn\"))\nget_sentiments(\"afinn\") %&gt;%\n  summary(value) \n  # summary statistics for the value column (from -5 to 5)\n\nget_sentiments(\"bing\")\n\n\n# Count words from the lexicons that appear in a text and add them all up\n\n# (positives - negatives)\ndata_token_stop &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text) %&gt;%   # tokenise\n  anti_join(stop_words, by = c(\"words\" = \"word\"))   # remove stop words\n\nsentiment &lt;- data_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"bing\"), relationship = \"many-to-many\") %&gt;%\n  count(sentiment, year = Year) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative) \n\nsentiment %&gt;%\n  ggplot(aes(year, sentiment)) +\n    geom_line(show.legend = FALSE) +\n    geom_hline(yintercept = 0, linetype = 2, alpha = .8)\n\nsentiment %&gt;%\n  filter(year &gt; 1950 & sentiment &gt; 400)\n\n# using the afinn lexicon\ndata_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"afinn\")) %&gt;%\n  group_by(Year) %&gt;%\n  summarize(sentiment = sum(value)) %&gt;%\n  ggplot(aes(Year, sentiment)) +\n    geom_line(show.legend = FALSE) +\n    geom_hline(yintercept = 0, linetype = 2, alpha = .8)\n\n\n# Putting both analysis together\n\nbing &lt;- data_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"bing\"), relationship = \"many-to-many\") %&gt;%\n  count(sentiment, year = Year) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(bing = positive - negative) \n\nafinn &lt;- data_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"afinn\")) %&gt;%\n  group_by(Year) %&gt;%\n  summarize(afinn = sum(value))\n\nbing %&gt;%\n  inner_join(afinn, by = c(\"year\" = \"Year\")) %&gt;%\n  pivot_longer(c(\"bing\", \"afinn\"), names_to = \"sentiment\", values_to = \"counts\") %&gt;%\n  ggplot(aes(x = year, y = counts, color = sentiment)) +\n    geom_line() +\n    geom_hline(yintercept = 0, linetype = 2, alpha = .8)\n\n\n\n# Topic models\n  # discovering the abstract \"topics\" that occur in a collection of documents\n  # unveils hidden semantic structures in a text body\n  # it works with matrixes of words/documents\n  # more details here: \n  # https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2\n\n# install.packages('topicmodels')\n# install.packages('stm')\n\noptions(stringsAsFactors = FALSE)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(stm)\nlibrary(SnowballC)\n\n# transform dataframe to DTM - document term matrix\ndata_dtm &lt;- data_token_stop %&gt;%\n  mutate(word_stem = wordStem(words)) %&gt;% # stemming\n  select(President, period, word_stem) %&gt;%\n  rename(President = President, words = word_stem) %&gt;%\n  group_by(period) %&gt;%\n  count(words, sort = TRUE) %&gt;%\n  cast_dtm(period, words, n)\ndata_dtm\n\n# transform to tm (adjust the parameters)\nk = 10      # number of topics\nalpha = 2   # how many topics may dominate each text\ndata_tm &lt;- LDA(data_dtm, k = 10, alpha = 2) \n  # LDA: Latent Dirichlet Allocation (algorithm)\n  # choosing the number of topics is a bit of an art\n\n?LDA\n\n# look at the output of the topic model\nstr(posterior(data_tm))\n\n# highest words in each topic (do the topics make sense for a human?)\nterms(data_tm, 15)\n\n# transform the output back to the tidy format (so we can better work with it)\n# install.packages(\"tidyr\")\n# install.packages(\"reshape2\")\nlibrary(tidytext)\nlibrary(tidyr)\n\nterms &lt;- tidy(data_tm, matrix = \"beta\")\nterms\n  # the topics are given numbers\n  # you can later \"name\" them \n\nwords_in_topics &lt;- terms %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\nwords_in_topics\n\nwords_in_topics %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    scale_y_reordered()\n\n\n\n\n#### Co-occurence\n  # Words that appear together (although not necessarily right next to each other)\n# install.packages(\"widyr\")\nlibrary(widyr)\n\ndata_adj &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text) %&gt;%   # tokenize\n  anti_join(stop_words, by = c(\"words\" = \"word\")) %&gt;%   # drop stop words\n  pairwise_count(words, Year, sort = TRUE)\n  # Count the number of times each pair of items appear together within a group\n\ndata_adj\n\n## Exercises:\n  # Analise the evolution of the importance of \"education\" (and related words) in the speeches\n  # What time of words show up in the context of education? Do they change over time?\n\n\nCorpuscle: corpuses\nhttps://clarino.uib.no/korpuskel/home"
  },
  {
    "objectID": "r-scripts/desc_text.html",
    "href": "r-scripts/desc_text.html",
    "title": "1.4. Textual information (counting words)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM-2024/session\") \n\n# Install packages\n# install.packages(\"tidyverse\")\n# install.packages(\"tidytext\")\n# install.packages(\"tokenizers\")\n\n# Open packages that we will be using\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Importing the data\ndata &lt;- read_csv(\"data/state_of_the_union_texts.csv\")\n\n# Having a look at the data\ndata\nprint(data)\nprint(data, n=20)\nprint(data, n=Inf)\nView(data)\n\n# In case it was not a tibble, convert to the tidyverse's format for dataframes\ndata &lt;- as_tibble(data)\n\ndata %&gt;% \n  count(President)\n\ndata %&gt;%\n  count(Year) %&gt;% print(n = Inf)\n\n\n# Reporting a particular speech (number 4 in this case)\ndata$Text[4] \n  # will show the fourth line of the text column of the \"data\" dataframe\n\n\n\n#### n-grams (number of particular words in the texts)\ndata &lt;- data %&gt;%\n  mutate(peace = str_count(Text, \"[Pp]eace\")) %&gt;%\n  mutate(war = str_count(Text, \"[Ww]ar\")) \n\n# summary statistics\ndata %&gt;%\n  summarize(obs = sum(!is.na(war)),\n            sum = sum(war),\n            mean = mean(war, na.rm = TRUE)) \n\ndata %&gt;%\n  group_by(Year) %&gt;%\n  summarize(sum = sum(war), \n            mean = mean(war, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = Year, y = mean)) +\n  geom_line()\n\ndata %&gt;%\n  filter(Year&gt;1790) %&gt;%\n  ggplot(aes(x = Year, y = war)) +\n  geom_line()\n\ndata %&gt;%\n  filter(war&gt;150)\n\n# aggregate the data into periods\ndata %&gt;%\n  mutate(period = case_when(\n    Year&lt;1914 ~ \"Pre-1914\",\n    Year&gt;=1914 & Year&lt;=1945 ~ \"World Wars\",\n    Year&gt;1945 ~ \"Post-1945\")) %&gt;%\n  mutate(period = factor(period, \n                         levels = c(\"Pre-1914\", \"World Wars\", \"Post-1945\"),\n                         ordered = TRUE)) %&gt;%\n  group_by(period) %&gt;%\n  summarize(sum = sum(war), \n            mean = mean(war, na.rm = TRUE)) \n  \n    # factor ranks them (instead of alphabetically, as with strings)\n\n# Plots\ndata %&gt;%\n  group_by(Year) %&gt;%\n  summarize(peace = sum(peace), war = sum(war)) %&gt;%\n  pivot_longer(c(\"peace\", \"war\"), names_to = \"word\", values_to = \"counts\") %&gt;%\n  ggplot(aes(x = Year, y = counts, color = word)) +\n  geom_line()\n\ndata %&gt;%\n  filter(Year&gt;1790) %&gt;%\n  pivot_longer(c(\"peace\", \"war\"), names_to = \"word\", values_to = \"counts\") %&gt;%  \n  ggplot(aes(x = Year, y = counts, color = word)) +\n  geom_line()\n\ndata %&gt;%\n  filter(Year&gt;1840 & Year&lt;1855 & war&gt;50)\n\n  # do it with \"[Ww]omen\"\ndata &lt;- data %&gt;% mutate(women = str_count(Text, \"[Ww]om[ae]n\"))\n\ndata %&gt;%\n  group_by(Year) %&gt;%\n  summarize(women = sum(women)) %&gt;%\n  ggplot(aes(x = Year, y = women)) +\n    geom_line()\n\n\ndata %&gt;%\n  filter(Year&gt;1790) %&gt;%\n  ggplot(aes(x = Year, y = women)) +\n  geom_line()\n  # no need to group by year if you exclude the first year (with 2 speeches)\n\n\n\n#### Compute \"word counts\"\n\n# Counting all words or numbers that are separated by spaces on either side\ndata &lt;- data %&gt;%\n  mutate(wc = str_count(Text, \"[\\\\w]+\"))\n  # [\\\\w]+ is a regular expression (regex or regexp)\n    # a sequence of characters that specifies a match pattern in text\n  # more on this here: https://regexr.com\n  # this other handy tool provides a quick references to regular expressions \n    # and also allows testing them: https://regex101.com\n\n# Graph it\ndata %&gt;%\n  ggplot(aes(x = Year, y = wc)) + \n  geom_point() + \n  geom_line()\n# just line graph\n# we might add geom_point() to underscore that we have missing years\n\ndata %&gt;% \n  filter(Year&gt;1940 & wc&gt;20000)\n\ndata %&gt;% \n  mutate(war_rel = war/wc) %&gt;%\n  mutate(war_rel2 = 1000*war/wc) %&gt;%\n  filter(Year&gt;1790) %&gt;%\n  ggplot(aes(x = Year, y = war_rel)) + \n  geom_point() + \n  geom_line()\n\n#### \"tokenizing\"  \n  # removes all of the punctuation, \n  # splits the text into individual words, and \n  # converts everything into lowercase characters\n\ndata_token &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text)\n\ndata_token # almost 1.8 million entries\n\n\n#### Top word frequencies (the most common words)\ndata_token %&gt;%\n  count(words, sort = TRUE) %&gt;% \n  print(n = 20) # \"Inf\" display all rows\n  # the most common words have no meaning\n\n#### stop words \n  # words that are so common that we are not interested in them \n  # already built within the tidy environment\n\nstop_words %&gt;% print(n = 25)\n  # note that the stop words are in the \"word\" column\n\n# Exclude stop_words from the tokens\ndata_token_stop &lt;- data_token %&gt;%\n  anti_join(stop_words, by = c(\"words\" = \"word\"))\n  # anti_join drops the words matching in both \"datasets\"\n  # \"words\" & \"word\" are the names of the fields in each \"dataset\"\n\ndata_token_stop %&gt;%\n  count(words, sort = TRUE) %&gt;% \n  print(n = 20)\n\n# Create our own stop words (or add more to the existing list)\nstop_words\nmy_stop_words &lt;- c(\"peace\", \"war\")\nmy_stop_words\ncustom_stop_words &lt;- tibble(word = my_stop_words, lexicon = \"my_customization\")\ncustom_stop_words\nstop_words_custom &lt;- rbind(stop_words, custom_stop_words)\nstop_words_custom\ntail(stop_words_custom) # view the end of the tibble, look like our words were added correctly\n\ndata_token_stop # reduce to almost 700,000 entries\n\n# Graphing - Top word frequencies\ndata_token_stop %&gt;%\n  count(words, sort=TRUE) %&gt;%\n  top_n(15) %&gt;%                     # selecting to show only top 15 words\n  mutate(words = reorder(words, n)) %&gt;%  # highest frequency words appear first\n  ggplot(aes(words, n)) +\n    geom_col() +\n    coord_flip()\n\n# Focusing on a particular period (1850-1900)\ndata_token_stop %&gt;%\n  filter(Year &gt;1850 & Year &lt; 1900) %&gt;%\n  count(words, sort=TRUE) %&gt;%\n  top_n(15) %&gt;%                     # selecting to show only top 15 words\n  mutate(words = reorder(words, n)) %&gt;%  # this will ensure that the highest frequency words appear to the left\n  ggplot(aes(words, n)) +\n  geom_col() +\n  coord_flip()\n\n# Comparing periods\ndata_token_stop &lt;- data_token_stop %&gt;%\n  mutate(period = ifelse(Year &lt;= 1900, \"19th c.\", \"20th c.\"))\n\ndata_token_stop %&gt;%\n  group_by(period) %&gt;%\n  count(words, sort=TRUE) %&gt;%\n  mutate(proportion = n / sum(n) * 1000) %&gt;%  # word freq- per 1000 words instead of counts\n  slice_max(order_by=proportion, n = 15) %&gt;%  # selecting to show only top 15 words\n  mutate(words = reorder(words, desc(proportion))) %&gt;%  # this will ensure that the highest frequency words appear to the left\n  ggplot(aes(reorder_within(x = words, by = proportion, within = period), proportion, fill = period)) +    # reordering is a bit tricky, see                                                                                                     ?reorder_within()\n    geom_col() +\n    scale_x_reordered() +\n    scale_fill_manual(values = c(\"blue\", \"#56B4E9\")) +\n    coord_flip() +\n    facet_wrap(~period, ncol = 2, scales = \"free\") +\n    xlab(\"Word\")\n\n?ggplot\n\n# Focusing on particular words\ndata_token_stop &lt;- data_token_stop %&gt;%\n  mutate(war = ifelse(words == \"war\", 1, 0))\n\ndata_token %&gt;%\n  filter(Year&gt;=1900 & Year&lt;2000) %&gt;%\n  mutate(women = ifelse(words == \"women\", 1, 0)) %&gt;%\n  count(women)\n\n# Evolution over time\ndata_token_stop %&gt;%   \n  group_by(Year) %&gt;%\n  summarize(fr_war = mean(war, na.rm = TRUE)) %&gt;%\n  ggplot() +\n    geom_col(aes(x = Year, y = fr_war))\n\n\n# Stemming\n  # looking at the common root of similar words\n\n# install.packages(SnowballC)\nlibrary(SnowballC)\n\ndata_token_stop_stem &lt;- data_token_stop %&gt;%\n  mutate(word_stem = wordStem(words))\ndata_token_stop_stem\n\ndata_token_stop_stem %&gt;%\n  count(word_stem, sort=TRUE) %&gt;% \n  print(n = 30)\n\ndata_token_stop_stem %&gt;%\n  count(word_stem, sort=TRUE) %&gt;%\n  top_n(15) %&gt;%                     # selecting to show only top 15 words\n  mutate(word_stem = reorder(word_stem, n)) %&gt;%  # this will ensure that the highest frequency words appear to the left\n  ggplot(aes(word_stem, n)) +\n  geom_col() + coord_flip()\n\n\n# We could do the whole process simultaneously\ndata_adj &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text) %&gt;%       # tokenize\n  anti_join(stop_words, by = c(\"words\" = \"word\")) %&gt;%   # drop stop words\n  mutate(word_stem = wordStem(words))\n\ndata_adj\n\ndata_adj &lt;- data_adj %&gt;%\n  mutate(war = ifelse(words == \"war\", 1, 0)) %&gt;%\n  mutate(peace = ifelse(words == \"peace\", 1, 0))\n\ndata_adj %&gt;%   \n  group_by(Year) %&gt;%\n  summarize(fr_war = mean(war, na.rm = TRUE),\n            fr_peace = mean(peace, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = Year)) +\n    geom_line(aes(y = fr_war), color = \"red\") +\n    geom_line(aes(y = fr_peace), color = \"blue\")\n\n\n#### n-grams: multiple-word tokens\n\ndata_token2 &lt;- data %&gt;%\n  unnest_tokens(twogram, Text, token = \"ngrams\", n = 2)\ndata_token2\n\n# in separate columns\ndata_token2 &lt;- data_token2 %&gt;%\n  separate_wider_delim(cols = twogram, delim = \" \", names = c(\"g1\", \"g2\"))\ndata_token2\n\n# identify words accompanying particular words\nwomen &lt;- data_token2 %&gt;%\n  filter(g1 == \"women\" | g2 == \"women\") %&gt;%\n  pivot_longer(g1:g2) %&gt;% # put both columns in the same one\n  select(!name) %&gt;% # drop the variable we are not using\n  rename(words = value) %&gt;% # rename the new column we created\n  filter(words!=\"women\") %&gt;% # drop the word women (we are interested in those around)\n  anti_join(stop_words, by = c(\"words\" = \"word\")) \n\nwomen %&gt;%\n  count(words, sort=TRUE) %&gt;%\n  print(n = 25)\n\nwomen %&gt;% \n  filter(words == \"pregnant\") %&gt;%\n  count(Year) \n\n\n# Exercise: \n  # think about a potential topic (word) \n  # explore the words around it (2 in each side)\n\n\n# Further topics:\n  # TF-IDF Term frequency - Inverse document frequency\n  # POS (Part of Speech) - nouns, verbs, adjectives...\n  # Sentiment analysis\n  # Topic models\n  # Co-occurrence\n\n\n\n# check women / men\n\ndata %&gt;%\n  mutate(women = str_count(Text, \"[Ww]omen\")) %&gt;%\n  summarize(women = sum(women)) # 300 \n\ndata_token %&gt;%\n  mutate(women = if_else(words, \"women\")) %&gt;%\n  summarize(women = sum(women)) # 302 \n\ndata_token2 %&gt;%\n  mutate(women = str_count(g2, \"[Ww]omen\")) %&gt;%\n  summarize(women = sum(women)) # 302 in either g1 and g2"
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Below are the instructions to prepare the session in advance:\n1. Install R and RStudio. R is a (free) statistical software and RStudio is an integrated interface that makes working with R easier. Bear in mind that you have to download and install both softwares: R and RStudio. The version you install depends on whether you are using Windows or Mac. Here is a link to access them.\n2. Create a dedicated folder to this session in your computer. Name it mapping.\n3. Download the following materials. Store these files in separate folders (named data and scripts, respectively) within the folder mapping:\n\nThe datasets we will be working with: here and here.\nThe R scripts containing the code to extract (some) information from them: here and here.\n\n4. Get familiar with R and RStudio by opening RStudio and following the instructions in the Intro to R section.\n5. Read the following texts in advance:\n\nThe background information for the two case-studies we will be exploring: The Paisley Prisoners’ Dataset and The State of the Union Presidential Speeches.\nThe article Quantifying history (available in Blackboard). This text outlines the advantages and disadvantages of using quantitative and computational methods in history. We will be discussing it at the beginning of the first session.\n\n6. Do not forget to bring your laptop to the session (or work in pairs)\nDo not hesitate to contact me if you have any question!"
  },
  {
    "objectID": "backg-tnp.html",
    "href": "backg-tnp.html",
    "title": "Case-study 3: The Tudor Network of Letters",
    "section": "",
    "text": "The third case-study deals with The Tudor Networks of Power data set (Ahnert et al. 2023), which contains a large corpus of letters originated from the correspondence kept in the Tudor State Papers, thus covering the period from the ascension of Henry VIII (1509) and the death of Elizabeth I (1603), an important period in British and European history.1\nThe letter below, for instance, dated in April 1554, was sent from Queen Mary I (“Marye the quene”) to inform Lord Paget of the imminent arrival of Phillip II, Prince of Spain, to England (more examples here). Lord Paget, an important statesman and diplomat, was crucial during the negotiations to secure a marriage agreement between Mary and Philip, who were married at Winchester Cathedral on July 1554.\nThese letters form part of the official government records of the Tudor period in England. In particular, they store the communications between the members of the government: the monarchs’ secretaries and the monarchs themselves, local government officials, overseas embassies and military missions. They also include intercepted and seized correspondence, as well as the petitions to government from across Tudor Society. The provide therefore a privileged window into the functioning of the Tudor governments and the society as a whole.\nIn total, there are almost 140,000 letters, exchanged between more than 20,000 historical actors. You can check the wonderful visualisation the authors made available here.] This digitised source that has opened up new ways of studying the Early Modern Period (Ahnert and Ahnert 2023). Given the scale of the dataset, we will work with a 1 per-cent random sample to reduce the computing requirements. The resulting data frame contains 1,393 rows, which in our case refer to letters. The image below shows how the first rows in this data set look like.\n# A tibble: 1,393 × 10\n   from   to    date_from date_to place_from y_from x_from place_to   x_to  y_to\n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Charl… The …  16010701  1.60e7 &lt;NA&gt;         NA    NA    &lt;NA&gt;     NA      NA  \n 2 T.B.   Sir …  15980101  1.60e7 &lt;NA&gt;         NA    NA    Court o… -0.138  51.5\n 3 Sir W… Priv…  15900910  1.59e7 Kilmainham   NA    NA    Greenwi…  0      51.5\n 4 Isaac… Quee…  15980228  1.60e7 Paris        48.9   2.35 Greenwi…  0      51.5\n 5 Queen… Offi…  15570709  1.56e7 &lt;NA&gt;         NA    NA    &lt;NA&gt;     NA      NA  \n 6 Willi… Sir …  15711026  1.57e7 Edinburgh    56.0  -3.19 Court o… -0.138  51.5\n 7 Queen… Thom…  15590111  1.56e7 &lt;NA&gt;         NA    NA    Newcast… -1.61   55.0\n 8 Queen… Robe…  15680204  1.57e7 &lt;NA&gt;         NA    NA    Dublin   -6.26   53.3\n 9 Adolf… Sir …  15880805  1.59e7 Utrecht      52.1   5.12 Paris     2.35   48.9\n10 Georg… John…  15330101  1.53e7 &lt;NA&gt;         NA    NA    &lt;NA&gt;     NA      NA  \n# ℹ 1,383 more rows\nThis source includes information on the sender (from) and the receiver (to), as well the dates these letters were sent and received (date_from and date_to). Note that the first two columns also define the direction of the relationship: from A to B. The same person can be therefore found as sender and/or recipient (he/she can also send/receive one or multiple letters to/from the same or different individuals).2 This dataset also indicates the locations of the sender and the recipient, as well as their subsequent spatial coordinates: longitude and latitudes (x_from and y_from; x_to and y_to).3 Mapping these locations allow having a sense of the reach of the Tudor governments (their worldview).\nGiven that these letters document the relationships between these historical actors, we will treat this data set as a social network. Explicitly analysing the relationships between these actors helps understanding the complex web of social and institutional relationships that shaped their world: the presence of distinct communities, the role that each member played within the wider network; the emergence, evolution and demise of particular communities; the features that made these networks different; etc. As the authors made perfectly clear, the letters that ended up in this data set constitute a selected sample of all existing letters, so any inference should be taken with caution."
  },
  {
    "objectID": "backg-tnp.html#footnotes",
    "href": "backg-tnp.html#footnotes",
    "title": "Case-study 3: The Tudor Network of Letters",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApart from these two monarchs, Edward VI (1547-1553) and Mary I (1553-1558) also reigned during this period.↩︎\nAlthough the data set we are exploring does not contain information on the content of the letters themselves, the original source includes a description of the letter contents Ahnert and Ahnert (2023, 147–204). This would constitute another layer. Here, we are just concern with the metadata (sender, recipient, place of origin, date, etc.) but this does not preclude the analysis of the texts themselves in different stages of the research process.↩︎\nThe original data only reports the sending location. We have curated it and completed the information assuming that those receiving the letters did so in the location that was most common when sending letters (we should be aware nonetheless that we are not sure where exactly the letter was dispatched to).↩︎"
  },
  {
    "objectID": "backg-paisley.html",
    "href": "backg-paisley.html",
    "title": "Case-study 1: The Paisley House of Correction Dataset",
    "section": "",
    "text": "The fist case-study deals with the admission records from the Paisley prison, an institution located in a village near Glasgow (The Scottish National Archives). Kept in the National Archives of Scotland, these registers provide individual information on those prisoners who were admitted into this institution between 1841 and 1883.\nFor an illustration on how this type of source looks like, see the register included below of John Hearn, a 12-year old convicted in 1873 for stealing 11 pieces of leather who was sentenced to one month of hard labour. Interestingly, prison records provide info on both males and females, a feature that is relatively uncommon for this period (when most of the height data come from military conscripts).\nThe previous picture comes from the H.M. Prison Wandsworth, near London. The Paisley records are not as fancy. Figure 2 below shows a sample of the original source. While each column records different pieces of information (case number, date of admission, name, etc.), each row refers to each inmate in the data set. The data set was originally collected by Hamish Maxwell-Stewart, James Bradley and Tamsin O’Connor who systematically sample every fourth double page of all registers, resulting in a total of 13,879 observations. For practical reasons, we will rely on a subset of the full data set, containing a thousand prisoners.1 This source presents information in a very structured form, that can be easily transferred to a digital version.\nAs well as the case number, the source reports name, sex, age, place (and country) of birth, place of residence, height, weight, occupation and literacy level, etc. As well demographic and socio-economic information, the registers also include the offence committed and the sentence they were punished with (see image below). The education, occupations, and types of crimes (mostly petty crimes) suggest that these prisoners were drawn from the low working classes, which constituted a large proportion of the full population at the time. Comparing the prisoners’ registers with those of the 1861 population census also confirms that they were “ordinary if vulnerable workers” (Meredith and Oxley 2015, 209).\nInputing the raw data into an Excel spreadsheet results in Figure 3 below. Each column, known as field or variable presents a piece of information. As well as the case number (casen) and the date of admission (information that is split in two fields: month and year), the source records several pieces of information about these inmates, such name and surname, sex, age, place of birth (born) and country of birth (countryb), the place where the were living before being imprisoned (reside), height (in feet and inches) and weight, occupation (occup) and whether they were employed or not. It also reports their literacy, the marks that were visible in their bodies, the offence they committed and the sentence they received. While the first row displays the name of these variables, the remaining rows are devoted to each individual in the dataset.\nWho were these prisoners? Where they were coming from? Did prisoners’ occupations differ significantly from the rest of the population? What about literacy rates? Did men and women commit different crimes? Did judges treat everyone equally or did particular groups suffer harsher sentences? What explains the variation in stature and weights observed across prisoners? How did theses dimensions change during the period? The range of historical questions that this source can address is almost endless. Sarah Horrell, David Meredith and Deborah Oxley have relied on this information to significantly contribute to our understanding of 19th-century British society, especially regarding the biological living standards of the working classes and the gender dynamics that drove the allocation of resources within these families [Horrell and Oxley (2013); Meredith and Oxley (2015)).2 We strongly encourage reading those pieces to get to know more about the source and its possibilities. Bear in mind that, for practical reasons, we will rely on a subset of the full data set, containing a thousand prisoners.3\nThis type of source allows exploring many historical questions:\nSarah Horrell, David Meredith and Deborah Oxley have relied on this information to significantly contribute to our understanding of 19th-century British society, especially regarding the living standards of the working classes and gender dynamics that drove the allocation of resources within these families (Horrell and Oxley 2013; Meredith and Oxley 2015). In a seminal article, Sarah Horrell and Deborah Oxley had previously addressed these issues using similar registers from the Wandsworth prison, near London (Horrell, Meredith, and Oxley 2009). Those who are interested in knowing more about the source and its possibilities are encouraged to read those articles."
  },
  {
    "objectID": "backg-paisley.html#footnotes",
    "href": "backg-paisley.html#footnotes",
    "title": "Case-study 1: The Paisley House of Correction Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am extremely thankful to Deborah Oxley and Hamish Maxwell-Stewart for kindly sharing this material. I am especially indebted to Deb whose course materials I inherited when I first started teaching a similar course at the University of Oxford in 2012.↩︎\nIn a seminal paper, Sarah Horrell and Deborah Oxley had previously addressed these issues using similar registers from the Wandsworth prison, near London (Horrell, Meredith, and Oxley 2009).↩︎\nWe are extremely grateful to Deborah Oxley and Hamish Maxwell-Stewart for kindly sharing the Paisley dataset.↩︎"
  },
  {
    "objectID": "further-readings.html",
    "href": "further-readings.html",
    "title": "Further reading",
    "section": "",
    "text": "For an expanded coverage how the topics discussed during the course can be implemented in R, see (pebesma2022?), (tennekes2022?), (bivand2023?), (gimond2023?), (moraga2023?) and (lovelace2024?).\nFor more details on GIS and mapping, see (crampton2010?) and (bolstad2023?).\nFor a more detailed discussion on how historians use GIS tools, see (gregory2007?), (knowles2008?), (marti2011?), (gregory2014?), (gregory2018?); (lawson2021?), (travis2020?), (cole2022?), (mogorovich2022?) and (mcdonough2024?). See also (presner2015?) for a discussion on the geospatial turn in the humanities in general. Spatial history nonetheless comes in a variety of forms, not necessarily involving using GIS tools (bavaj2022b?). Maps actually constitute another historical source, so they contain useful information that helps knowing more about the past. In this regard, historians of maps and mapping practices often study maps as cultural objects (mcdonough2024?). (schulten2012?) and (spychal2024?), for instance, trace the growing use of maps in the US and UK during the 19th century as tools for shaping history, policy and national identity.\nOnline visualisations:\n\nUS frontier expansion of post offices during the 19th century\nRacial lynchings in the US, 1877-1950\nOceanic shipping from 18th- and 19th-century ship’s logs\nMapping the State of the Union"
  }
]